{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-28 15:22:38,991] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01818934, -0.01173276, -0.04890503, -0.02562905])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-28 15:22:40,075] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_episodes = 0\n",
    "reward_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis = .99\n",
    "num_episodes = 2000\n",
    "learning_rate = 0.1\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-12-e9122ee5edab>\", line 4, in <module>\n    W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n  File \"/Users/essemfly/.pyenv/versions/3.6.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/essemfly/.pyenv/versions/3.6.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fcd5e8724e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mQpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mQpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    662\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 664\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-12-e9122ee5edab>\", line 4, in <module>\n    W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n  File \"/Users/essemfly/.pyenv/versions/3.6.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/essemfly/.pyenv/versions/3.6.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "Qpred = tf.matmul(X, W1)\n",
    "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(Y-Qpred))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00922436  0.19520776  0.02105398 -0.2595297 ] 1.0 False\n",
      "[-0.00532021  0.39002292  0.01586339 -0.54549826] 1.0 False\n",
      "[ 0.00248025  0.1946817   0.00495342 -0.24785966] 1.0 False\n",
      "[  6.37388554e-03  -5.10638032e-04  -3.76829280e-06   4.63815596e-02] 1.0 False\n",
      "[ 0.00636367 -0.19563254  0.00092386  0.3390633 ] 1.0 False\n",
      "[ 0.00245102 -0.39076762  0.00770513  0.63203742] 1.0 False\n",
      "[-0.00536433 -0.19575401  0.02034588  0.34179097] 1.0 False\n",
      "[-0.00927941 -0.00092735  0.0271817   0.05559274] 1.0 False\n",
      "[-0.00929796  0.19379452  0.02829355 -0.22839174] 1.0 False\n",
      "[-0.00542207 -0.00172009  0.02372572  0.07308   ] 1.0 False\n",
      "[-0.00545647  0.19305383  0.02518732 -0.21202395] 1.0 False\n",
      "[-0.00159539  0.38780677  0.02094684 -0.49665642] 1.0 False\n",
      "[ 0.00616074  0.58262719  0.01101371 -0.78266502] 1.0 False\n",
      "[ 0.01781329  0.77759605 -0.00463959 -1.07186261] 1.0 False\n",
      "[ 0.03336521  0.97277904 -0.02607684 -1.36599797] 1.0 False\n",
      "[ 0.05282079  1.16821761 -0.0533968  -1.66672188] 1.0 False\n",
      "[ 0.07618514  1.36391846 -0.08673124 -1.97554596] 1.0 False\n",
      "[ 0.10346351  1.16981088 -0.12624216 -1.71094604] 1.0 False\n",
      "[ 0.12685973  0.97634508 -0.16046108 -1.46007342] 1.0 False\n",
      "[ 0.14638663  0.78351296 -0.18966255 -1.22151438] 1.0 False\n",
      "[ 0.16205689  0.5912718  -0.21409284 -0.99375066] 1.0 True\n",
      "Reward for this episode was: 21.0\n",
      "[ 0.03416961  0.16234632 -0.03265276 -0.27880523] 1.0 False\n",
      "[ 0.03741654  0.3579185  -0.03822886 -0.58160532] 1.0 False\n",
      "[ 0.04457491  0.16335245 -0.04986097 -0.30120604] 1.0 False\n",
      "[ 0.04784196  0.3591483  -0.05588509 -0.60918786] 1.0 False\n",
      "[ 0.05502493  0.16485029 -0.06806885 -0.3346175 ] 1.0 False\n",
      "[ 0.05832193 -0.0292402  -0.0747612  -0.06415277] 1.0 False\n",
      "[ 0.05773713  0.16686959 -0.07604425 -0.37945509] 1.0 False\n",
      "[ 0.06107452 -0.02709473 -0.08363335 -0.1116849 ] 1.0 False\n",
      "[ 0.06053262  0.16911985 -0.08586705 -0.42953761] 1.0 False\n",
      "[ 0.06391502 -0.02468782 -0.0944578  -0.16511107] 1.0 False\n",
      "[ 0.06342127  0.17165045 -0.09776003 -0.48603492] 1.0 False\n",
      "[ 0.06685427 -0.02196593 -0.10748072 -0.22569274] 1.0 False\n",
      "[ 0.06641496 -0.21540075 -0.11199458  0.03124664] 1.0 False\n",
      "[ 0.06210694 -0.40875332 -0.11136965  0.2866009 ] 1.0 False\n",
      "[ 0.05393187 -0.60212538 -0.10563763  0.54218699] 1.0 False\n",
      "[ 0.04188937 -0.40568972 -0.09479389  0.2181763 ] 1.0 False\n",
      "[ 0.03377557 -0.20934952 -0.09043036 -0.10283946] 1.0 False\n",
      "[ 0.02958858 -0.01305579 -0.09248715 -0.42262726] 1.0 False\n",
      "[ 0.02932747 -0.20675409 -0.1009397  -0.16047405] 1.0 False\n",
      "[ 0.02519238 -0.0103428  -0.10414918 -0.48321727] 1.0 False\n",
      "[ 0.02498553 -0.20385253 -0.11381352 -0.22509004] 1.0 False\n",
      "[ 0.02090848 -0.00730349 -0.11831532 -0.55139515] 1.0 False\n",
      "[ 0.02076241 -0.20058238 -0.12934323 -0.29820726] 1.0 False\n",
      "[ 0.01675076 -0.39364615 -0.13530737 -0.04895162] 1.0 False\n",
      "[ 0.00887784 -0.5865946  -0.1362864   0.19816662] 1.0 False\n",
      "[-0.00285405 -0.77953073 -0.13232307  0.444943  ] 1.0 False\n",
      "[-0.01844467 -0.5828092  -0.12342421  0.11365073] 1.0 False\n",
      "[-0.03010085 -0.38615473 -0.1211512  -0.21528168] 1.0 False\n",
      "[-0.03782395 -0.18952785 -0.12545683 -0.54359152] 1.0 False\n",
      "[-0.0416145  -0.38268417 -0.13632866 -0.2929218 ] 1.0 False\n",
      "[-0.04926819 -0.18590852 -0.1421871  -0.62530322] 1.0 False\n",
      "[-0.05298636 -0.37878932 -0.15469316 -0.38056402] 1.0 False\n",
      "[-0.06056215 -0.57141514 -0.16230444 -0.14037417] 1.0 False\n",
      "[-0.07199045 -0.37438573 -0.16511193 -0.47954239] 1.0 False\n",
      "[-0.07947816 -0.17736496 -0.17470277 -0.81937695] 1.0 False\n",
      "[-0.08302546  0.01966241 -0.19109031 -1.16152109] 1.0 False\n",
      "[-0.08263221 -0.17252796 -0.21432073 -0.93432225] 1.0 True\n",
      "Reward for this episode was: 37.0\n",
      "[-0.01980241  0.15365513 -0.01671863 -0.27025548] 1.0 False\n",
      "[-0.0167293   0.34901162 -0.02212374 -0.56816431] 1.0 False\n",
      "[-0.00974907  0.15420686 -0.03348702 -0.28253253] 1.0 False\n",
      "[-0.00666493  0.34979006 -0.03913767 -0.58558619] 1.0 False\n",
      "[ 0.00033087  0.15523753 -0.0508494  -0.30548441] 1.0 False\n",
      "[ 0.00343562  0.35104584 -0.05695909 -0.61376052] 1.0 False\n",
      "[ 0.01045653  0.54691553 -0.0692343  -0.9238255 ] 1.0 False\n",
      "[ 0.02139484  0.35279371 -0.08771081 -0.65367899] 1.0 False\n",
      "[ 0.02845072  0.15899551 -0.10078439 -0.3898531 ] 1.0 False\n",
      "[ 0.03163063  0.3553927  -0.10858145 -0.71253444] 1.0 False\n",
      "[ 0.03873848  0.55183717 -0.12283214 -1.03732606] 1.0 False\n",
      "[ 0.04977523  0.35854285 -0.14357866 -0.78559101] 1.0 False\n",
      "[ 0.05694608  0.55531481 -0.15929048 -1.11977911] 1.0 False\n",
      "[ 0.06805238  0.75212642 -0.18168606 -1.45789269] 1.0 False\n",
      "[ 0.08309491  0.94895217 -0.21084391 -1.80139414] 1.0 True\n",
      "Reward for this episode was: 15.0\n",
      "[ 0.04848204  0.18315218 -0.01409587 -0.31495651] 1.0 False\n",
      "[ 0.05214509  0.37847206 -0.020395   -0.61205127] 1.0 False\n",
      "[ 0.05971453  0.57387302 -0.03263602 -0.91108747] 1.0 False\n",
      "[ 0.07119199  0.37920753 -0.05085777 -0.62883801] 1.0 False\n",
      "[ 0.07877614  0.18483084 -0.06343453 -0.3525957 ] 1.0 False\n",
      "[ 0.08247276  0.38079478 -0.07048645 -0.66458766] 1.0 False\n",
      "[ 0.09008865  0.18672046 -0.0837782  -0.3949049 ] 1.0 False\n",
      "[ 0.09382306  0.38292503 -0.0916763  -0.71278152] 1.0 False\n",
      "[ 0.10148156  0.18918393 -0.10593193 -0.45030533] 1.0 False\n",
      "[ 0.10526524 -0.00429279 -0.11493803 -0.19280284] 1.0 False\n",
      "[ 0.10517939  0.19226974 -0.11879409 -0.51941864] 1.0 False\n",
      "[ 0.10902478  0.38884616 -0.12918246 -0.84704853] 1.0 False\n",
      "[ 0.1168017   0.5854713  -0.14612343 -1.17740277] 1.0 False\n",
      "[ 0.12851113  0.78215713 -0.16967149 -1.51209497] 1.0 False\n",
      "[ 0.14415427  0.97887889 -0.19991339 -1.85258473] 1.0 False\n",
      "[ 0.16373185  0.78643858 -0.23696508 -1.6280571 ] 1.0 True\n",
      "Reward for this episode was: 16.0\n",
      "[-0.00941478  0.20702307  0.04173702 -0.27211548] 1.0 False\n",
      "[-0.00527432  0.40152538  0.03629471 -0.55134775] 1.0 False\n",
      "[ 0.00275619  0.59611928  0.02526775 -0.83237807] 1.0 False\n",
      "[ 0.01467857  0.40066132  0.00862019 -0.53185678] 1.0 False\n",
      "[ 0.0226918   0.20541919 -0.00201695 -0.23647016] 1.0 False\n",
      "[ 0.02680018  0.4005699  -0.00674635 -0.52978861] 1.0 False\n",
      "[ 0.03481158  0.5957861  -0.01734212 -0.82458967] 1.0 False\n",
      "[ 0.0467273   0.4009056  -0.03383392 -0.53741119] 1.0 False\n",
      "[ 0.05474542  0.5964865  -0.04458214 -0.84055992] 1.0 False\n",
      "[ 0.06667515  0.79218781 -0.06139334 -1.14692302] 1.0 False\n",
      "[ 0.0825189   0.98805533 -0.0843318  -1.45820909] 1.0 False\n",
      "[ 0.10228001  0.79406307 -0.11349598 -1.19301898] 1.0 False\n",
      "[ 0.11816127  0.6005792  -0.13735636 -0.93795664] 1.0 False\n",
      "[ 0.13017285  0.40754973 -0.15611549 -0.69139545] 1.0 False\n",
      "[ 0.13832385  0.6044536  -0.1699434  -1.02887108] 1.0 False\n",
      "[ 0.15041292  0.41195046 -0.19052082 -0.79399929] 1.0 False\n",
      "[ 0.15865193  0.21988273 -0.20640081 -0.56678554] 1.0 False\n",
      "[ 0.16304958  0.02816246 -0.21773652 -0.34556098] 1.0 True\n",
      "Reward for this episode was: 18.0\n",
      "[-0.04866686 -0.16312255 -0.03121339  0.23671736] 1.0 False\n",
      "[-0.05192932 -0.35778498 -0.02647905  0.51939353] 1.0 False\n",
      "[-0.05908502 -0.55252434 -0.01609118  0.80361624] 1.0 False\n",
      "[ -7.01355021e-02  -3.57185499e-01  -1.88520278e-05   5.05915305e-01] 1.0 False\n",
      "[-0.07727921 -0.16206328  0.01009945  0.21322644] 1.0 False\n",
      "[-0.08052048  0.03291283  0.01436398 -0.07625363] 1.0 False\n",
      "[-0.07986222 -0.16241206  0.01283891  0.22092635] 1.0 False\n",
      "[-0.08311046 -0.35771516  0.01725744  0.51763138] 1.0 False\n",
      "[-0.09026477 -0.16284039  0.02761006  0.23043619] 1.0 False\n",
      "[-0.09352157  0.03187637  0.03221879 -0.05341129] 1.0 False\n",
      "[-0.09288405  0.22652189  0.03115056 -0.33575736] 1.0 False\n",
      "[-0.08835361  0.03097078  0.02443542 -0.03341622] 1.0 False\n",
      "[-0.08773419  0.22573395  0.02376709 -0.31829047] 1.0 False\n",
      "[-0.08321951  0.0302817   0.01740128 -0.01820802] 1.0 False\n",
      "[-0.08261388  0.22514983  0.01703712 -0.30535025] 1.0 False\n",
      "[-0.07811088  0.02978929  0.01093012 -0.00734329] 1.0 False\n",
      "[-0.0775151   0.2247528   0.01078325 -0.29655769] 1.0 False\n",
      "[-0.07302004  0.41971938  0.0048521  -0.58582035] 1.0 False\n",
      "[-0.06462565  0.61477303 -0.00686431 -0.87697087] 1.0 False\n",
      "[-0.05233019  0.41974504 -0.02440373 -0.58645387] 1.0 False\n",
      "[-0.04393529  0.61520014 -0.0361328  -0.8867232 ] 1.0 False\n",
      "[-0.03163129  0.42058681 -0.05386727 -0.60561448] 1.0 False\n",
      "[-0.02321955  0.22625785 -0.06597956 -0.33037317] 1.0 False\n",
      "[-0.0186944   0.42225394 -0.07258702 -0.64311154] 1.0 False\n",
      "[-0.01024932  0.22821475 -0.08544925 -0.37414104] 1.0 False\n",
      "[-0.00568502  0.42443998 -0.09293207 -0.69249649] 1.0 False\n",
      "[ 0.00280378  0.2307218  -0.106782   -0.4304558 ] 1.0 False\n",
      "[ 0.00741821  0.03726117 -0.11539112 -0.17325201] 1.0 False\n",
      "[ 0.00816344  0.23382931 -0.11885616 -0.49999316] 1.0 False\n",
      "[ 0.01284002  0.04056584 -0.12885602 -0.24700467] 1.0 False\n",
      "[ 0.01365134 -0.1525029  -0.13379612  0.00241857] 1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01060128 -0.34547757 -0.13374774  0.25007475] 1.0 False\n",
      "[ 0.00369173 -0.53846122 -0.12874625  0.49776024] 1.0 False\n",
      "[-0.00707749 -0.73155523 -0.11879105  0.74725716] 1.0 False\n",
      "[-0.0217086  -0.92485544 -0.1038459   1.0003216 ] 1.0 False\n",
      "[-0.04020571 -1.11844775 -0.08383947  1.25866986] 1.0 False\n",
      "[-0.06257466 -1.31240286 -0.05866607  1.52396069] 1.0 False\n",
      "[-0.08882272 -1.50676934 -0.02818686  1.79777091] 1.0 False\n",
      "[-0.11895811 -1.31134359  0.00776856  1.49646288] 1.0 False\n",
      "[-0.14518498 -1.50655911  0.03769782  1.79156127] 1.0 False\n",
      "[-0.17531616 -1.31187951  0.07352904  1.51082986] 1.0 False\n",
      "[-0.20155375 -1.11772135  0.10374564  1.24197762] 1.0 False\n",
      "[-0.22390818 -1.31401063  0.12858519  1.56527498] 1.0 False\n",
      "[-0.25018839 -1.12063866  0.15989069  1.31531161] 1.0 False\n",
      "[-0.27260116 -0.92785972  0.18619692  1.07663947] 1.0 False\n",
      "[-0.29115836 -0.73561953  0.20772971  0.8476895 ] 1.0 False\n",
      "[-0.30587075 -0.93287715  0.2246835   1.19784913] 1.0 True\n",
      "Reward for this episode was: 47.0\n",
      "[-0.02709767 -0.17142029  0.01723985  0.30161927] 1.0 False\n",
      "[-0.03052607  0.02345176  0.02327224  0.0144229 ] 1.0 False\n",
      "[-0.03005704  0.21823236  0.0235607  -0.27082746] 1.0 False\n",
      "[-0.02569239  0.41301032  0.01814415 -0.5559871 ] 1.0 False\n",
      "[-0.01743218  0.60787289  0.00702441 -0.84289876] 1.0 False\n",
      "[-0.00527473  0.41265578 -0.00983357 -0.54801516] 1.0 False\n",
      "[ 0.00297839  0.60791448 -0.02079387 -0.84378008] 1.0 False\n",
      "[ 0.01513668  0.41308238 -0.03766947 -0.55770806] 1.0 False\n",
      "[ 0.02339833  0.60871233 -0.04882363 -0.86201702] 1.0 False\n",
      "[ 0.03557257  0.41428797 -0.06606397 -0.58507645] 1.0 False\n",
      "[ 0.04385833  0.22015058 -0.0777655  -0.31391429] 1.0 False\n",
      "[ 0.04826135  0.02621758 -0.08404379 -0.04673398] 1.0 False\n",
      "[ 0.0487857  -0.16760497 -0.08497847  0.21829387] 1.0 False\n",
      "[ 0.0454336   0.02862246 -0.08061259 -0.09993875] 1.0 False\n",
      "[ 0.04600605  0.22480165 -0.08261137 -0.41692635] 1.0 False\n",
      "[ 0.05050208  0.42099124 -0.09094989 -0.73446725] 1.0 False\n",
      "[ 0.0589219   0.61724411 -0.10563924 -1.05433227] 1.0 False\n",
      "[ 0.07126679  0.81359575 -0.12672588 -1.37821805] 1.0 False\n",
      "[ 0.0875387   0.62026359 -0.15429024 -1.12770307] 1.0 False\n",
      "[ 0.09994397  0.81703263 -0.17684431 -1.46453204] 1.0 False\n",
      "[ 0.11628463  0.6244623  -0.20613495 -1.23190325] 1.0 False\n",
      "[ 0.12877387  0.82155031 -0.23077301 -1.58145192] 1.0 True\n",
      "Reward for this episode was: 22.0\n",
      "[-0.03726114 -0.22664143 -0.03845785  0.25449254] 1.0 False\n",
      "[-0.04179397 -0.03099207 -0.033368   -0.05006832] 1.0 False\n",
      "[-0.04241381 -0.22562007 -0.03436936  0.23190279] 1.0 False\n",
      "[-0.04692622 -0.0300243  -0.02973131 -0.0714202 ] 1.0 False\n",
      "[-0.0475267   0.165511   -0.03115971 -0.37333324] 1.0 False\n",
      "[-0.04421648 -0.02915477 -0.03862637 -0.09063579] 1.0 False\n",
      "[-0.04479958 -0.2237024  -0.04043909  0.18961465] 1.0 False\n",
      "[-0.04927363 -0.41822318 -0.0366468   0.46927142] 1.0 False\n",
      "[-0.05763809 -0.22260322 -0.02726137  0.16526674] 1.0 False\n",
      "[-0.06209015 -0.02710185 -0.02395603 -0.1358902 ] 1.0 False\n",
      "[-0.06263219  0.1683549  -0.02667384 -0.4360336 ] 1.0 False\n",
      "[-0.05926509 -0.0263795  -0.03539451 -0.15187708] 1.0 False\n",
      "[-0.05979268 -0.22097723 -0.03843205  0.12943288] 1.0 False\n",
      "[-0.06421223 -0.02532642 -0.03584339 -0.17512288] 1.0 False\n",
      "[-0.06471876 -0.21991755 -0.03934585  0.10604064] 1.0 False\n",
      "[-0.06911711 -0.41445421 -0.03722504  0.3860551 ] 1.0 False\n",
      "[-0.07740619 -0.21882413 -0.02950394  0.08187163] 1.0 False\n",
      "[-0.08178267 -0.41351099 -0.0278665   0.36510193] 1.0 False\n",
      "[-0.09005289 -0.60822607 -0.02056447  0.64886946] 1.0 False\n",
      "[-0.10221741 -0.8030556  -0.00758708  0.93500643] 1.0 False\n",
      "[-0.11827853 -0.60783213  0.01111305  0.63994908] 1.0 False\n",
      "[-0.13043517 -0.80310724  0.02391203  0.93611083] 1.0 False\n",
      "[-0.14649731 -0.6083158   0.04263425  0.65103667] 1.0 False\n",
      "[-0.15866363 -0.80400481  0.05565498  0.95683413] 1.0 False\n",
      "[-0.17474373 -0.6096737   0.07479167  0.68214291] 1.0 False\n",
      "[-0.1869372  -0.80575025  0.08843452  0.99740377] 1.0 False\n",
      "[-0.20305221 -0.6119149   0.1083826   0.73375281] 1.0 False\n",
      "[-0.2152905  -0.41844399  0.12305766  0.47705141] 1.0 False\n",
      "[-0.22365938 -0.61506898  0.13259868  0.80584628] 1.0 False\n",
      "[-0.23596076 -0.81173508  0.14871561  1.13712571] 1.0 False\n",
      "[-0.25219546 -0.61883757  0.17145812  0.89453459] 1.0 False\n",
      "[-0.26457222 -0.81581779  0.18934882  1.23583452] 1.0 False\n",
      "[-0.28088857 -0.62356484  0.21406551  1.00794579] 1.0 True\n",
      "Reward for this episode was: 33.0\n",
      "[-0.00123449 -0.14743731  0.02496158  0.33883782] 1.0 False\n",
      "[-0.00418323  0.04732073  0.03173834  0.05412972] 1.0 False\n",
      "[-0.00323682 -0.14824159  0.03282093  0.35665497] 1.0 False\n",
      "[-0.00620165 -0.34381442  0.03995403  0.65950385] 1.0 False\n",
      "[-0.01307794 -0.53946898  0.05314411  0.96449482] 1.0 False\n",
      "[-0.02386732 -0.73526302  0.072434    1.27338827] 1.0 False\n",
      "[-0.03857258 -0.54113616  0.09790177  1.00423851] 1.0 False\n",
      "[-0.0493953  -0.34744854  0.11798654  0.74383547] 1.0 False\n",
      "[-0.05634427 -0.54398413  0.13286325  1.0711969 ] 1.0 False\n",
      "[-0.06722396 -0.35084499  0.15428719  0.82298846] 1.0 False\n",
      "[-0.07424086 -0.54770298  0.17074695  1.15994851] 1.0 False\n",
      "[-0.08519492 -0.35516618  0.19394593  0.92529911] 1.0 False\n",
      "[-0.09229824 -0.16311746  0.21245191  0.69929032] 1.0 True\n",
      "Reward for this episode was: 13.0\n",
      "[-0.02652496 -0.20566236  0.04705636  0.34209615] 1.0 False\n",
      "[-0.0306382  -0.01124038  0.05389828  0.06461546] 1.0 False\n",
      "[-0.03086301 -0.20709201  0.05519059  0.37380471] 1.0 False\n",
      "[-0.03500485 -0.40295274  0.06266668  0.68336627] 1.0 False\n",
      "[-0.04306391 -0.59888631  0.07633401  0.99510168] 1.0 False\n",
      "[-0.05504163 -0.79494165  0.09623604  1.31074873] 1.0 False\n",
      "[-0.07094047 -0.99114166  0.12245102  1.63193672] 1.0 False\n",
      "[-0.0907633  -0.79765219  0.15508975  1.37978643] 1.0 False\n",
      "[-0.10671634 -0.60476881  0.18268548  1.13934775] 1.0 False\n",
      "[-0.11881172 -0.80174706  0.20547244  1.48330964] 1.0 False\n",
      "[-0.13484666 -0.99869569  0.23513863  1.832503  ] 1.0 True\n",
      "Reward for this episode was: 11.0\n",
      "[ 0.02437836 -0.18282111  0.01806079  0.33556835] 1.0 False\n",
      "[ 0.02072194 -0.37819537  0.02477215  0.63389154] 1.0 False\n",
      "[ 0.01315803 -0.18342758  0.03744999  0.34911175] 1.0 False\n",
      "[ 0.00948948  0.01114228  0.04443222  0.06846932] 1.0 False\n",
      "[ 0.00971232 -0.1845876   0.04580161  0.37483315] 1.0 False\n",
      "[ 0.00602057 -0.38032919  0.05329827  0.68159859] 1.0 False\n",
      "[-0.00158601 -0.18598638  0.06693024  0.4061606 ] 1.0 False\n",
      "[-0.00530574 -0.38199043  0.07505345  0.71917121] 1.0 False\n",
      "[-0.01294555 -0.1879828   0.08943688  0.45102401] 1.0 False\n",
      "[-0.0167052   0.00576795  0.09845736  0.18781886] 1.0 False\n",
      "[-0.01658985  0.19935351  0.10221374 -0.07225344] 1.0 False\n",
      "[-0.01260278  0.39287285  0.10076867 -0.33101887] 1.0 False\n",
      "[-0.00474532  0.58642682  0.09414829 -0.59030114] 1.0 False\n",
      "[ 0.00698322  0.78011322  0.08234227 -0.85190536] 1.0 False\n",
      "[ 0.02258548  0.97402185  0.06530416 -1.11760151] 1.0 False\n",
      "[ 0.04206592  1.16822891  0.04295213 -1.38910538] 1.0 False\n",
      "[ 0.0654305   1.36279021  0.01517002 -1.66805411] 1.0 False\n",
      "[ 0.0926863   1.55773253 -0.01819106 -1.95597413] 1.0 False\n",
      "[ 0.12384095  1.75304268 -0.05731054 -2.25423877] 1.0 False\n",
      "[ 0.15890181  1.55850358 -0.10239532 -1.97974929] 1.0 False\n",
      "[ 0.19007188  1.36459727 -0.1419903  -1.72046494] 1.0 False\n",
      "[ 0.21736382  1.56103181 -0.1763996  -2.05375648] 1.0 False\n",
      "[ 0.24858446  1.75746667 -0.21747473 -2.39542926] 1.0 True\n",
      "Reward for this episode was: 23.0\n",
      "[-0.02408475  0.15677577  0.03208541 -0.23509511] 1.0 False\n",
      "[-0.02094924  0.35142496  0.02738351 -0.51748712] 1.0 False\n",
      "[-0.01392074  0.15592836  0.01703376 -0.21630242] 1.0 False\n",
      "[-0.01080217 -0.03943291  0.01270771  0.08170465] 1.0 False\n",
      "[-0.01159083  0.1555046   0.01434181 -0.20694202] 1.0 False\n",
      "[-0.00848074 -0.03981947  0.01020297  0.09023029] 1.0 False\n",
      "[-0.00927713 -0.23508617  0.01200757  0.38611471] 1.0 False\n",
      "[-0.01397885 -0.04013672  0.01972987  0.09724176] 1.0 False\n",
      "[-0.01478159 -0.2355358   0.0216747   0.39608355] 1.0 False\n",
      "[-0.0194923  -0.04072798  0.02959637  0.11031231] 1.0 False\n",
      "[-0.02030686 -0.23626126  0.03180262  0.41218385] 1.0 False\n",
      "[-0.02503209 -0.04160423  0.0400463   0.12969434] 1.0 False\n",
      "[-0.02586417 -0.23727628  0.04264018  0.43473756] 1.0 False\n",
      "[-0.0306097  -0.43297513  0.05133493  0.74055144] 1.0 False\n",
      "[-0.0392692  -0.2385981   0.06614596  0.46445582] 1.0 False\n",
      "[-0.04404116 -0.43458935  0.07543508  0.77723253] 1.0 False\n",
      "[-0.05273295 -0.63066315  0.09097973  1.0926637 ] 1.0 False\n",
      "[-0.06534621 -0.82685837  0.112833    1.41245055] 1.0 False\n",
      "[-0.08188338 -1.02318355  0.14108202  1.73816827] 1.0 False\n",
      "[-0.10234705 -1.21960345  0.17584538  2.07121144] 1.0 False\n",
      "[-0.12673912 -1.02665162  0.21726961  1.83767948] 1.0 True\n",
      "Reward for this episode was: 21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01451276 -0.23591549 -0.01639955  0.30552487] 1.0 False\n",
      "[ 0.00979445 -0.04056373 -0.01028905  0.00771538] 1.0 False\n",
      "[ 0.00898318 -0.23553662 -0.01013474  0.29713431] 1.0 False\n",
      "[ 0.00427245 -0.43051264 -0.00419206  0.58660375] 1.0 False\n",
      "[-0.00433781 -0.23533223  0.00754002  0.29260325] 1.0 False\n",
      "[-0.00904445 -0.04031859  0.01339208  0.00230785] 1.0 False\n",
      "[-0.00985082  0.15460876  0.01343824 -0.28611981] 1.0 False\n",
      "[-0.00675865  0.34953651  0.00771584 -0.57453431] 1.0 False\n",
      "[  2.32083789e-04   1.54307243e-01  -3.77484386e-03  -2.79430689e-01] 1.0 False\n",
      "[ 0.00331823 -0.04076066 -0.00936346  0.01205927] 1.0 False\n",
      "[ 0.00250302  0.15449432 -0.00912227 -0.28356317] 1.0 False\n",
      "[ 0.0055929  -0.04049634 -0.01479354  0.00622874] 1.0 False\n",
      "[ 0.00478298  0.15483461 -0.01466896 -0.29108477] 1.0 False\n",
      "[ 0.00787967  0.35016262 -0.02049066 -0.58835778] 1.0 False\n",
      "[ 0.01488292  0.54556543 -0.03225781 -0.8874243 ] 1.0 False\n",
      "[ 0.02579423  0.74111002 -0.0500063  -1.19007074] 1.0 False\n",
      "[ 0.04061643  0.54667052 -0.07380771 -0.9134718 ] 1.0 False\n",
      "[ 0.05154984  0.35262042 -0.09207715 -0.64486889] 1.0 False\n",
      "[ 0.05860225  0.54889672 -0.10497453 -0.96506861] 1.0 False\n",
      "[ 0.06958018  0.35532952 -0.1242759  -0.70712198] 1.0 False\n",
      "[ 0.07668677  0.16212831 -0.13841834 -0.45599835] 1.0 False\n",
      "[ 0.07992934  0.35890802 -0.1475383  -0.78890993] 1.0 False\n",
      "[ 0.0871075   0.55571486 -0.1633165  -1.12413207] 1.0 False\n",
      "[ 0.0982218   0.75255637 -0.18579914 -1.46326732] 1.0 False\n",
      "[ 0.11327292  0.56013274 -0.21506449 -1.2339108 ] 1.0 True\n",
      "Reward for this episode was: 25.0\n",
      "[ 0.0485569  -0.18797967 -0.04562663  0.26623303] 1.0 False\n",
      "[ 0.04479731 -0.38242173 -0.04030197  0.54418302] 1.0 False\n",
      "[ 0.03714887 -0.1867573  -0.02941831  0.23907912] 1.0 False\n",
      "[ 0.03341373  0.00877229 -0.02463673 -0.0627361 ] 1.0 False\n",
      "[ 0.03358917 -0.18598792 -0.02589145  0.2220731 ] 1.0 False\n",
      "[ 0.02986941  0.00949435 -0.02144999 -0.07866327] 1.0 False\n",
      "[ 0.0300593  -0.18531364 -0.02302325  0.20717569] 1.0 False\n",
      "[ 0.02635303 -0.38009894 -0.01887974  0.49250796] 1.0 False\n",
      "[ 0.01875105 -0.57494957 -0.00902958  0.77918151] 1.0 False\n",
      "[ 0.00725206 -0.37970465  0.00655405  0.4836714 ] 1.0 False\n",
      "[-0.00034204 -0.1846758   0.01622748  0.1930613 ] 1.0 False\n",
      "[-0.00403555 -0.38002609  0.02008871  0.49081881] 1.0 False\n",
      "[-0.01163607 -0.18519319  0.02990508  0.20453412] 1.0 False\n",
      "[-0.01533994 -0.38072976  0.03399577  0.50649861] 1.0 False\n",
      "[-0.02295453 -0.18610292  0.04412574  0.22471987] 1.0 False\n",
      "[-0.02667659 -0.38182684  0.04862014  0.53098873] 1.0 False\n",
      "[-0.03431313 -0.57759777  0.05923991  0.8385868 ] 1.0 False\n",
      "[-0.04586508 -0.38333263  0.07601165  0.56510659] 1.0 False\n",
      "[-0.05353174 -0.5794341   0.08731378  0.88073535] 1.0 False\n",
      "[-0.06512042 -0.77562675  0.10492848  1.19954089] 1.0 False\n",
      "[-0.08063295 -0.97193791  0.1289193   1.52318048] 1.0 False\n",
      "[-0.10007171 -1.16835978  0.15938291  1.85316562] 1.0 False\n",
      "[-0.12343891 -1.36483514  0.19644622  2.19080373] 1.0 False\n",
      "[-0.15073561 -1.17208194  0.2402623   1.96461937] 1.0 True\n",
      "Reward for this episode was: 24.0\n",
      "[ 0.01869636  0.20355375  0.02433633 -0.28224575] 1.0 False\n",
      "[ 0.02276743  0.39832027  0.01869142 -0.56715485] 1.0 False\n",
      "[ 0.03073384  0.59317511  0.00734832 -0.85389109] 1.0 False\n",
      "[ 0.04259734  0.78819613 -0.0097295  -1.14425435] 1.0 False\n",
      "[ 0.05836126  0.98344383 -0.03261459 -1.43997246] 1.0 False\n",
      "[ 0.07803014  1.17895201 -0.06141404 -1.74266576] 1.0 False\n",
      "[ 0.10160918  1.37471672 -0.09626735 -2.0538036 ] 1.0 False\n",
      "[ 0.12910351  1.18070247 -0.13734343 -1.7923886 ] 1.0 False\n",
      "[ 0.15271756  0.98736161 -0.1731912  -1.54536043] 1.0 False\n",
      "[ 0.17246479  0.79469126 -0.20409841 -1.3113425 ] 1.0 False\n",
      "[ 0.18835862  0.60265297 -0.23032526 -1.08885317] 1.0 True\n",
      "Reward for this episode was: 11.0\n",
      "[-0.04514102  0.213134   -0.03684915 -0.30864871] 1.0 False\n",
      "[-0.04087834  0.4087611  -0.04302212 -0.61272134] 1.0 False\n",
      "[-0.03270312  0.60445705 -0.05527655 -0.91863825] 1.0 False\n",
      "[-0.02061398  0.80028091 -0.07364931 -1.22816843] 1.0 False\n",
      "[-0.00460836  0.60617999 -0.09821268 -0.95943967] 1.0 False\n",
      "[ 0.00751524  0.80247526 -0.11740148 -1.28129178] 1.0 False\n",
      "[ 0.02356474  0.60902818 -0.14302731 -1.0275554 ] 1.0 False\n",
      "[ 0.03574531  0.41606962 -0.16357842 -0.78297981] 1.0 False\n",
      "[ 0.0440667   0.61301633 -0.17923802 -1.12233413] 1.0 False\n",
      "[ 0.05632703  0.80997696 -0.2016847  -1.46545634] 1.0 False\n",
      "[ 0.07252657  1.00691561 -0.23099382 -1.81377066] 1.0 True\n",
      "Reward for this episode was: 11.0\n",
      "[ 0.00844651 -0.1713455   0.04338114  0.29337488] 1.0 False\n",
      "[ 0.0050196   0.02313196  0.04924864  0.01468319] 1.0 False\n",
      "[ 0.00548224  0.2175143   0.0495423  -0.26206354] 1.0 False\n",
      "[ 0.00983252  0.41189534  0.04430103 -0.53871787] 1.0 False\n",
      "[ 0.01807043  0.21617951  0.03352668 -0.23241191] 1.0 False\n",
      "[ 0.02239402  0.41080676  0.02887844 -0.51433373] 1.0 False\n",
      "[ 0.03061016  0.60551036  0.01859176 -0.79777827] 1.0 False\n",
      "[ 0.04272036  0.80037235  0.0026362  -1.08455509] 1.0 False\n",
      "[ 0.05872781  0.99545942 -0.0190549  -1.37640964] 1.0 False\n",
      "[ 0.078637    1.19081418 -0.0465831  -1.67499037] 1.0 False\n",
      "[ 0.10245328  0.99626274 -0.0800829  -1.39717027] 1.0 False\n",
      "[ 0.12237854  0.8022229  -0.10802631 -1.13056254] 1.0 False\n",
      "[ 0.13842299  0.99858062 -0.13063756 -1.45508022] 1.0 False\n",
      "[ 0.15839461  0.80528189 -0.15973917 -1.20590105] 1.0 False\n",
      "[ 0.17450024  0.6125432  -0.18385719 -0.96723756] 1.0 False\n",
      "[ 0.18675111  0.4203023  -0.20320194 -0.73748629] 1.0 False\n",
      "[ 0.19515715  0.61756399 -0.21795166 -1.08662206] 1.0 True\n",
      "Reward for this episode was: 17.0\n",
      "[-0.02364286 -0.16934964 -0.00979803  0.2902274 ] 1.0 False\n",
      "[-0.02702985  0.02591064 -0.00399348 -0.00552954] 1.0 False\n",
      "[-0.02651164 -0.16915381 -0.00410407  0.28589073] 1.0 False\n",
      "[-0.02989472  0.02602643  0.00161375 -0.00808376] 1.0 False\n",
      "[-0.02937419 -0.16911863  0.00145207  0.28510788] 1.0 False\n",
      "[-0.03275656  0.02598258  0.00715423 -0.00711672] 1.0 False\n",
      "[-0.03223691  0.22100121  0.00701189 -0.29753384] 1.0 False\n",
      "[-0.02781689  0.02578001  0.00106122 -0.00264776] 1.0 False\n",
      "[-0.02730129  0.22088672  0.00100826 -0.29499567] 1.0 False\n",
      "[-0.02288355  0.02575041 -0.00489165 -0.00199492] 1.0 False\n",
      "[-0.02236854 -0.16930105 -0.00493155  0.28914062] 1.0 False\n",
      "[-0.02575456 -0.36435233  0.00085126  0.58026412] 1.0 False\n",
      "[-0.03304161 -0.16924232  0.01245654  0.28784948] 1.0 False\n",
      "[-0.03642646 -0.36453967  0.01821353  0.58443491] 1.0 False\n",
      "[-0.04371725 -0.16967753  0.02990223  0.29754466] 1.0 False\n",
      "[-0.0471108  -0.3652127   0.03585313  0.59950624] 1.0 False\n",
      "[-0.05441506 -0.17061022  0.04784325  0.31832868] 1.0 False\n",
      "[-0.05782726  0.02379883  0.05420982  0.04110934] 1.0 False\n",
      "[-0.05735128 -0.17205688  0.05503201  0.35039123] 1.0 False\n",
      "[-0.06079242  0.02224099  0.06203984  0.07555687] 1.0 False\n",
      "[-0.0603476   0.21642121  0.06355097 -0.19692508] 1.0 False\n",
      "[-0.05601918  0.02045054  0.05961247  0.11510894] 1.0 False\n",
      "[-0.05561017  0.21466991  0.06191465 -0.15818695] 1.0 False\n",
      "[-0.05131677  0.4088533   0.05875091 -0.43071263] 1.0 False\n",
      "[-0.0431397   0.60309624  0.05013666 -0.70431151] 1.0 False\n",
      "[-0.03107778  0.79748889  0.03605043 -0.98080008] 1.0 False\n",
      "[-0.015128    0.9921096   0.01643443 -1.26194493] 1.0 False\n",
      "[ 0.00471419  0.79678139 -0.00880447 -0.96416068] 1.0 False\n",
      "[ 0.02064982  0.60177882 -0.02808769 -0.67425665] 1.0 False\n",
      "[ 0.0326854   0.79727964 -0.04157282 -0.9756489 ] 1.0 False\n",
      "[ 0.04863099  0.99293379 -0.0610858  -1.28109545] 1.0 False\n",
      "[ 0.06848967  1.18877847 -0.08670771 -1.5922626 ] 1.0 False\n",
      "[ 0.09226524  1.38481594 -0.11855296 -1.91067424] 1.0 False\n",
      "[ 0.11996155  1.1911553  -0.15676644 -1.65699527] 1.0 False\n",
      "[ 0.14378466  1.38772007 -0.18990635 -1.99412759] 1.0 False\n",
      "[ 0.17153906  1.19502672 -0.2297889  -1.76578142] 1.0 True\n",
      "Reward for this episode was: 36.0\n",
      "[-0.02016949 -0.20815663 -0.05048924  0.23139426] 1.0 False\n",
      "[-0.02433262 -0.01235094 -0.04586136 -0.07677752] 1.0 False\n",
      "[-0.02457964 -0.20678646 -0.04739691  0.20109058] 1.0 False\n",
      "[-0.02871537 -0.01101981 -0.0433751  -0.10615909] 1.0 False\n",
      "[-0.02893576 -0.2054942  -0.04549828  0.17252984] 1.0 False\n",
      "[-0.03304565 -0.00975158 -0.04204768 -0.1341521 ] 1.0 False\n",
      "[-0.03324068  0.18594664 -0.04473072 -0.43979836] 1.0 False\n",
      "[-0.02952175 -0.00851465 -0.05352669 -0.16154464] 1.0 False\n",
      "[-0.02969204  0.18733111 -0.05675758 -0.47062187] 1.0 False\n",
      "[-0.02594542  0.38320692 -0.06617002 -0.78064024] 1.0 False\n",
      "[-0.01828128  0.18905398 -0.08178282 -0.50948795] 1.0 False\n",
      "[-0.0145002   0.38522711 -0.09197258 -0.82678148] 1.0 False\n",
      "[-0.00679566  0.19147509 -0.10850821 -0.56438363] 1.0 False\n",
      "[-0.00296616 -0.00197053 -0.11979589 -0.3077606 ] 1.0 False\n",
      "[-0.00300557 -0.19519993 -0.1259511  -0.0551296 ] 1.0 False\n",
      "[-0.00690957 -0.388312   -0.12705369  0.19531214] 1.0 False\n",
      "[-0.01467581 -0.19162325 -0.12314745 -0.13459625] 1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01850827 -0.3847858  -0.12583937  0.11683941] 1.0 False\n",
      "[-0.02620399 -0.57790111 -0.12350258  0.36732263] 1.0 False\n",
      "[-0.03776201 -0.38126047 -0.11615613  0.0383908 ] 1.0 False\n",
      "[-0.04538722 -0.57454173 -0.11538832  0.29228587] 1.0 False\n",
      "[-0.05687805 -0.7678456  -0.1095426   0.54646458] 1.0 False\n",
      "[-0.07223497 -0.96127178 -0.09861331  0.80272366] 1.0 False\n",
      "[-0.0914604  -0.76494586 -0.08255883  0.48072017] 1.0 False\n",
      "[-0.10675932 -0.9588113  -0.07294443  0.74628311] 1.0 False\n",
      "[-0.12593554 -0.76276272 -0.05801877  0.43156561] 1.0 False\n",
      "[-0.1411908  -0.9570172  -0.04938746  0.70540911] 1.0 False\n",
      "[-0.16033114 -1.15142132 -0.03527927  0.98214571] 1.0 False\n",
      "[-0.18335957 -1.34605321 -0.01563636  1.26354192] 1.0 False\n",
      "[-0.21028063 -1.54097181  0.00963448  1.55128719] 1.0 False\n",
      "[-0.24110007 -1.34596673  0.04066022  1.26162564] 1.0 False\n",
      "[-0.2680194  -1.15138767  0.06589274  0.98194909] 1.0 False\n",
      "[-0.29104716 -0.95720764  0.08553172  0.71066959] 1.0 False\n",
      "[-0.31019131 -1.15340341  0.09974511  1.0290031 ] 1.0 False\n",
      "[-0.33325938 -1.34970109  0.12032517  1.35126255] 1.0 False\n",
      "[-0.3602534  -1.15627826  0.14735042  1.09851638] 1.0 False\n",
      "[-0.38337896 -0.96337048  0.16932075  0.8554548 ] 1.0 False\n",
      "[-0.40264637 -1.16034501  0.18642985  1.1962341 ] 1.0 False\n",
      "[-0.42585327 -0.96805982  0.21035453  0.96729755] 1.0 True\n",
      "Reward for this episode was: 39.0\n",
      "[-0.04811171  0.16138006 -0.02751067 -0.26750848] 1.0 False\n",
      "[-0.04488411 -0.03333869 -0.03286084  0.016372  ] 1.0 False\n",
      "[-0.04555088  0.16223874 -0.0325334  -0.28649511] 1.0 False\n",
      "[-0.04230611  0.35780919 -0.0382633  -0.58925869] 1.0 False\n",
      "[-0.03514992  0.16324333 -0.05004848 -0.30887018] 1.0 False\n",
      "[-0.03188506  0.35904134 -0.05622588 -0.61690755] 1.0 False\n",
      "[-0.02470423  0.16474809 -0.06856403 -0.34244992] 1.0 False\n",
      "[-0.02140927  0.3607751  -0.07541303 -0.6559416 ] 1.0 False\n",
      "[-0.01419377  0.55686142 -0.08853186 -0.97138553] 1.0 False\n",
      "[-0.00305654  0.363032   -0.10795957 -0.70777444] 1.0 False\n",
      "[ 0.0042041   0.16955805 -0.12211506 -0.4509316 ] 1.0 False\n",
      "[ 0.00759526 -0.02364436 -0.13113369 -0.19909877] 1.0 False\n",
      "[ 0.00712238 -0.21667072 -0.13511567  0.04951196] 1.0 False\n",
      "[ 0.00278896 -0.01989609 -0.13412543 -0.28256305] 1.0 False\n",
      "[ 0.00239104  0.17685857 -0.13977669 -0.61435911] 1.0 False\n",
      "[ 0.00592821 -0.01606208 -0.15206387 -0.36876108] 1.0 False\n",
      "[ 0.00560697 -0.20873329 -0.15943909 -0.12762393] 1.0 False\n",
      "[ 0.0014323  -0.01172917 -0.16199157 -0.4660588 ] 1.0 False\n",
      "[ 0.00119772  0.18526642 -0.17131275 -0.8051011 ] 1.0 False\n",
      "[ 0.00490305 -0.00714509 -0.18741477 -0.57082862] 1.0 False\n",
      "[ 0.00476015 -0.19921309 -0.19883134 -0.34254946] 1.0 False\n",
      "[ 0.00077588 -0.39103308 -0.20568233 -0.11856032] 1.0 False\n",
      "[-0.00704478 -0.19364907 -0.20805354 -0.46844074] 1.0 False\n",
      "[-0.01091776 -0.38531845 -0.21742235 -0.24786415] 1.0 True\n",
      "Reward for this episode was: 24.0\n",
      "[-0.04818139  0.17254408  0.00675273 -0.26056969] 1.0 False\n",
      "[-0.04473051 -0.02267361  0.00154133  0.03423547] 1.0 False\n",
      "[-0.04518398 -0.21781763  0.00222604  0.3274043 ] 1.0 False\n",
      "[-0.04954034 -0.02272744  0.00877413  0.0354242 ] 1.0 False\n",
      "[-0.04999489 -0.21797411  0.00948261  0.33086249] 1.0 False\n",
      "[-0.05435437 -0.41322975  0.01609986  0.62652063] 1.0 False\n",
      "[-0.06261896 -0.60857269  0.02863028  0.92423022] 1.0 False\n",
      "[-0.07479042 -0.41384893  0.04711488  0.64068044] 1.0 False\n",
      "[-0.0830674  -0.21941437  0.05992849  0.3631989 ] 1.0 False\n",
      "[-0.08745568 -0.4153346   0.06719247  0.6741601 ] 1.0 False\n",
      "[-0.09576238 -0.61132287  0.08067567  0.98721884] 1.0 False\n",
      "[-0.10798883 -0.807427    0.10042005  1.30411121] 1.0 False\n",
      "[-0.12413737 -0.61371167  0.12650227  1.04447599] 1.0 False\n",
      "[-0.13641161 -0.81026535  0.14739179  1.37404314] 1.0 False\n",
      "[-0.15261691 -1.00688984  0.17487265  1.70895847] 1.0 False\n",
      "[-0.17275471 -0.81415615  0.20905182  1.47541802] 1.0 False\n",
      "[-0.18903783 -1.01112812  0.23856018  1.82545785] 1.0 True\n",
      "Reward for this episode was: 17.0\n",
      "[ 0.01723136  0.23369065  0.03635465 -0.23252055] 1.0 False\n",
      "[ 0.02190518  0.03806861  0.03170424  0.07140454] 1.0 False\n",
      "[ 0.02266655 -0.15749319  0.03313233  0.37391931] 1.0 False\n",
      "[ 0.01951669  0.03714283  0.04061072  0.09186463] 1.0 False\n",
      "[ 0.02025954  0.23165988  0.04244801 -0.18773411] 1.0 False\n",
      "[ 0.02489274  0.03595714  0.03869333  0.11803155] 1.0 False\n",
      "[ 0.02561188 -0.15969723  0.04105396  0.42266643] 1.0 False\n",
      "[ 0.02241794  0.03481979  0.04950729  0.14320322] 1.0 False\n",
      "[ 0.02311433  0.22919905  0.05237135 -0.13345923] 1.0 False\n",
      "[ 0.02769832  0.42353324  0.04970217 -0.4091707 ] 1.0 False\n",
      "[ 0.03616898  0.22774313  0.04151875 -0.1012418 ] 1.0 False\n",
      "[ 0.04072384  0.03205151  0.03949392  0.20424568] 1.0 False\n",
      "[ 0.04136487 -0.16361232  0.04357883  0.50912076] 1.0 False\n",
      "[ 0.03809263 -0.35932027  0.05376124  0.8152121 ] 1.0 False\n",
      "[ 0.03090622 -0.55513554  0.07006549  1.12430883] 1.0 False\n",
      "[ 0.01980351 -0.36099843  0.09255166  0.85440006] 1.0 False\n",
      "[ 0.01258354 -0.55725167  0.10963966  1.17469137] 1.0 False\n",
      "[  1.43850784e-03  -7.53614112e-01   1.33133492e-01   1.49963600e+00] 1.0 False\n",
      "[-0.01363377 -0.95007838  0.16312621  1.83075029] 1.0 False\n",
      "[-0.03263534 -1.14658823  0.19974122  2.16934858] 1.0 False\n",
      "[-0.05556711 -1.34302234  0.24312819  2.51647569] 1.0 True\n",
      "Reward for this episode was: 21.0\n",
      "[ 0.0207811   0.2206639  -0.00148131 -0.27703834] 1.0 False\n",
      "[ 0.02519438  0.41580696 -0.00702208 -0.5701881 ] 1.0 False\n",
      "[ 0.03351052  0.61102668 -0.01842584 -0.86507494] 1.0 False\n",
      "[ 0.04573105  0.80639452 -0.03572734 -1.16349385] 1.0 False\n",
      "[ 0.06185894  1.00196298 -0.05899722 -1.46716094] 1.0 False\n",
      "[ 0.0818982   0.80761079 -0.08834044 -1.19347499] 1.0 False\n",
      "[ 0.09805042  0.61373701 -0.11220994 -0.92973664] 1.0 False\n",
      "[ 0.11032516  0.81018008 -0.13080467 -1.25546867] 1.0 False\n",
      "[ 0.12652876  1.00671142 -0.15591404 -1.5860943 ] 1.0 False\n",
      "[ 0.14666299  1.20330537 -0.18763593 -1.92306143] 1.0 False\n",
      "[ 0.1707291   1.39988206 -0.22609716 -2.26759283] 1.0 True\n",
      "Reward for this episode was: 11.0\n",
      "[ 0.01356282  0.17194019 -0.04745393 -0.32483574] 1.0 False\n",
      "[ 0.01700162  0.36770456 -0.05395064 -0.63209795] 1.0 False\n",
      "[ 0.02435571  0.56353605 -0.0665926  -0.94127159] 1.0 False\n",
      "[ 0.03562643  0.75948921 -0.08541803 -1.2541136 ] 1.0 False\n",
      "[ 0.05081622  0.5655587  -0.1105003  -0.98936079] 1.0 False\n",
      "[ 0.06212739  0.37207547 -0.13028752 -0.73332702] 1.0 False\n",
      "[ 0.0695689   0.17897129 -0.14495406 -0.48432197] 1.0 False\n",
      "[ 0.07314833  0.37580953 -0.1546405  -0.81895023] 1.0 False\n",
      "[ 0.08066452  0.57267155 -0.1710195  -1.15600285] 1.0 False\n",
      "[ 0.09211795  0.76955949 -0.19413956 -1.49706138] 1.0 False\n",
      "[ 0.10750914  0.96643825 -0.22408079 -1.84355085] 1.0 True\n",
      "Reward for this episode was: 11.0\n",
      "[-0.01564453 -0.23493862 -0.02420135  0.30554639] 1.0 False\n",
      "[-0.0203433  -0.42970748 -0.01809042  0.59049963] 1.0 False\n",
      "[-0.02893745 -0.23433698 -0.00628043  0.29217353] 1.0 False\n",
      "[ -3.36241942e-02  -4.29368822e-01  -4.36958523e-04   5.82869088e-01] 1.0 False\n",
      "[-0.04221157 -0.23424075  0.01122042  0.29004854] 1.0 False\n",
      "[-0.04689639 -0.03928058  0.01702139  0.00092543] 1.0 False\n",
      "[-0.047682    0.15559318  0.0170399  -0.28633881] 1.0 False\n",
      "[-0.04457013  0.35046802  0.01131313 -0.57359915] 1.0 False\n",
      "[ -3.75607732e-02   5.45429545e-01  -1.58856580e-04  -8.62696736e-01] 1.0 False\n",
      "[-0.02665218  0.35030976 -0.01741279 -0.57006376] 1.0 False\n",
      "[-0.01964599  0.15543628 -0.02881407 -0.28291697] 1.0 False\n",
      "[-0.01653726 -0.03926309 -0.03447241  0.0005407 ] 1.0 False\n",
      "[-0.01732252 -0.23387413 -0.03446159  0.28215095] 1.0 False\n",
      "[-0.02200001 -0.03827801 -0.02881857 -0.02119874] 1.0 False\n",
      "[-0.02276557 -0.23297507 -0.02924255  0.2622541 ] 1.0 False\n",
      "[-0.02742507 -0.42766766 -0.02399747  0.54557205] 1.0 False\n",
      "[-0.03597842 -0.62244435 -0.01308602  0.83059838] 1.0 False\n",
      "[-0.04842731 -0.81738501  0.00352594  1.11913716] 1.0 False\n",
      "[-0.06477501 -0.6223095   0.02590869  0.82756233] 1.0 False\n",
      "[-0.0772212  -0.4275512   0.04245993  0.54313924] 1.0 False\n",
      "[-0.08577222 -0.23305088  0.05332272  0.26413118] 1.0 False\n",
      "[-0.09043324 -0.03872897  0.05860534 -0.01126795] 1.0 False\n",
      "[-0.09120782  0.15550567  0.05837998 -0.2848996 ] 1.0 False\n",
      "[-0.08809771 -0.04039822  0.05268199  0.02560957] 1.0 False\n",
      "[-0.08890567  0.1539302   0.05319418 -0.2499973 ] 1.0 False\n",
      "[-0.08582707 -0.04190944  0.04819424  0.05897836] 1.0 False\n",
      "[-0.08666525 -0.23768809  0.0493738   0.36646897] 1.0 False\n",
      "[-0.09141902 -0.04330126  0.05670318  0.08975405] 1.0 False\n",
      "[-0.09228504  0.15096402  0.05849826 -0.18451373] 1.0 False\n",
      "[-0.08926576  0.34520229  0.05480799 -0.45818408] 1.0 False\n",
      "[-0.08236172  0.53950834  0.04564431 -0.73310002] 1.0 False\n",
      "[-0.07157155  0.7339709   0.03098231 -1.01107528] 1.0 False\n",
      "[-0.05689213  0.53844952  0.0107608  -0.70882663] 1.0 False\n",
      "[-0.04612314  0.34318019 -0.00341573 -0.41277597] 1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03925954  0.53835039 -0.01167125 -0.70653379] 1.0 False\n",
      "[-0.02849253  0.34339206 -0.02580193 -0.41754749] 1.0 False\n",
      "[-0.02162469  0.14864508 -0.03415288 -0.13310917] 1.0 False\n",
      "[-0.01865179  0.34423918 -0.03681506 -0.43636822] 1.0 False\n",
      "[-0.011767    0.5398624  -0.04554242 -0.74042541] 1.0 False\n",
      "[ -9.69754544e-04   7.35582565e-01  -6.03509325e-02  -1.04708609e+00] 1.0 False\n",
      "[ 0.0137419   0.54131123 -0.08129265 -0.77394202] 1.0 False\n",
      "[ 0.02456812  0.34739611 -0.09677149 -0.50790365] 1.0 False\n",
      "[ 0.03151604  0.54373883 -0.10692957 -0.82944621] 1.0 False\n",
      "[ 0.04239082  0.74014731 -0.12351849 -1.15375367] 1.0 False\n",
      "[ 0.05719377  0.54683354 -0.14659357 -0.90221438] 1.0 False\n",
      "[ 0.06813044  0.35396906 -0.16463785 -0.65896486] 1.0 False\n",
      "[ 0.07520982  0.16147473 -0.17781715 -0.42231295] 1.0 False\n",
      "[ 0.07843931  0.35861134 -0.18626341 -0.76535842] 1.0 False\n",
      "[ 0.08561154  0.55574272 -0.20157058 -1.11038617] 1.0 False\n",
      "[ 0.09672639  0.36375617 -0.2237783  -0.88709823] 1.0 True\n",
      "Reward for this episode was: 50.0\n",
      "[ 0.0075161   0.19549238 -0.02530322 -0.29370185] 1.0 False\n",
      "[ 0.01142595  0.00074015 -0.03117726 -0.00910538] 1.0 False\n",
      "[ 0.01144075  0.19629503 -0.03135937 -0.31145978] 1.0 False\n",
      "[ 0.01536665  0.00163356 -0.03758856 -0.02882928] 1.0 False\n",
      "[ 0.01539933  0.19727383 -0.03816515 -0.33313083] 1.0 False\n",
      "[ 0.0193448   0.39291762 -0.04482777 -0.63760065] 1.0 False\n",
      "[ 0.02720315  0.58863507 -0.05757978 -0.94405685] 1.0 False\n",
      "[ 0.03897586  0.78448348 -0.07646092 -1.25426171] 1.0 False\n",
      "[ 0.05466553  0.59041944 -0.10154615 -0.98647375] 1.0 False\n",
      "[ 0.06647391  0.39679317 -0.12127562 -0.7273338 ] 1.0 False\n",
      "[ 0.07440978  0.203538   -0.1358223  -0.4751479 ] 1.0 False\n",
      "[ 0.07848054  0.4002902  -0.14532526 -0.80736725] 1.0 False\n",
      "[ 0.08648634  0.59707306 -0.1614726  -1.14200546] 1.0 False\n",
      "[ 0.0984278   0.79389391 -0.18431271 -1.48066317] 1.0 False\n",
      "[ 0.11430568  0.99072398 -0.21392598 -1.82478919] 1.0 True\n",
      "Reward for this episode was: 15.0\n",
      "[ 0.01272402 -0.20846106 -0.0133367   0.26873396] 1.0 False\n",
      "[ 0.0085548  -0.40339017 -0.00796202  0.55718075] 1.0 False\n",
      "[  4.86998869e-04  -5.98399442e-01   3.18159470e-03   8.47344581e-01] 1.0 False\n",
      "[-0.01148099 -0.79356465  0.02012849  1.1410263 ] 1.0 False\n",
      "[-0.02735228 -0.98894384  0.04294901  1.43995309] 1.0 False\n",
      "[-0.04713116 -1.18456774  0.07174807  1.74574147] 1.0 False\n",
      "[-0.07082251 -0.99033113  0.1066629   1.47621199] 1.0 False\n",
      "[-0.09062914 -0.79666157  0.13618714  1.21865809] 1.0 False\n",
      "[-0.10656237 -0.99325073  0.1605603   1.55072682] 1.0 False\n",
      "[-0.12642738 -0.80037771  0.19157484  1.31214059] 1.0 False\n",
      "[-0.14243494 -0.99733808  0.21781765  1.65815536] 1.0 True\n",
      "Reward for this episode was: 11.0\n",
      "[ 0.01835085 -0.15390611  0.04493703  0.29064642] 1.0 False\n",
      "[ 0.01527273  0.04054724  0.05074996  0.01246789] 1.0 False\n",
      "[ 0.01608367 -0.15526442  0.05099932  0.32072129] 1.0 False\n",
      "[ 0.01297839  0.03909556  0.05741375  0.04454769] 1.0 False\n",
      "[ 0.0137603  -0.15680068  0.0583047   0.35477824] 1.0 False\n",
      "[ 0.01062428 -0.35270108  0.06540027  0.66526139] 1.0 False\n",
      "[ 0.00357026 -0.54866883  0.07870549  0.97779857] 1.0 False\n",
      "[-0.00740311 -0.74475285  0.09826146  1.29412961] 1.0 False\n",
      "[-0.02229817 -0.5510072   0.12414406  1.03375542] 1.0 False\n",
      "[-0.03331832 -0.74754173  0.14481917  1.3626931 ] 1.0 False\n",
      "[-0.04826915 -0.94415063  0.17207303  1.69694747] 1.0 False\n",
      "[-0.06715216 -0.75138053  0.20601198  1.46240277] 1.0 False\n",
      "[-0.08217977 -0.55929153  0.23526003  1.24050201] 1.0 True\n",
      "Reward for this episode was: 13.0\n",
      "[ 0.00265679 -0.2410503   0.01407419  0.248042  ] 1.0 False\n",
      "[-0.00216421 -0.4363704   0.01903503  0.5451308 ] 1.0 False\n",
      "[-0.01089162 -0.24152102  0.02993765  0.25850564] 1.0 False\n",
      "[-0.01572204 -0.43705729  0.03510776  0.56047898] 1.0 False\n",
      "[-0.02446319 -0.24244521  0.04631734  0.2790603 ] 1.0 False\n",
      "[-0.02931209 -0.43819625  0.05189854  0.58598438] 1.0 False\n",
      "[-0.03807602 -0.24383815  0.06361823  0.3100911 ] 1.0 False\n",
      "[-0.04295278 -0.4398061   0.06982005  0.62213952] 1.0 False\n",
      "[-0.0517489  -0.24572499  0.08226284  0.35223757] 1.0 False\n",
      "[-0.0566634  -0.05186333  0.0893076   0.08658645] 1.0 False\n",
      "[-0.05770067  0.14187251  0.09103933 -0.17663763] 1.0 False\n",
      "[-0.05486322 -0.05442639  0.08750657  0.14331995] 1.0 False\n",
      "[-0.05595174  0.13934045  0.09037297 -0.12052409] 1.0 False\n",
      "[-0.05316494 -0.05695227  0.08796249  0.19924693] 1.0 False\n",
      "[-0.05430398  0.13680865  0.09194743 -0.06444314] 1.0 False\n",
      "[-0.05156781  0.33050025  0.09065857 -0.32675879] 1.0 False\n",
      "[-0.0449578   0.13421239  0.08412339 -0.00691901] 1.0 False\n",
      "[-0.04227356 -0.06200896  0.08398501  0.31107529] 1.0 False\n",
      "[-0.04351373 -0.25822078  0.09020652  0.62901822] 1.0 False\n",
      "[-0.04867815 -0.06446579  0.10278688  0.36605217] 1.0 False\n",
      "[-0.04996747  0.12905674  0.11010792  0.10746662] 1.0 False\n",
      "[-0.04738633 -0.06745672  0.11225726  0.43275812] 1.0 False\n",
      "[-0.04873547 -0.2639742   0.12091242  0.75861331] 1.0 False\n",
      "[-0.05401495 -0.46053639  0.13608468  1.08676564] 1.0 False\n",
      "[-0.06322568 -0.65716487  0.15782     1.41886707] 1.0 False\n",
      "[-0.07636898 -0.85384862  0.18619734  1.7564329 ] 1.0 False\n",
      "[-0.09344595 -1.05052924  0.221326    2.10078073] 1.0 True\n",
      "Reward for this episode was: 27.0\n",
      "[ -8.20926423e-03   2.14821009e-01   1.49315641e-04  -2.92709517e-01] 1.0 False\n",
      "[ -3.91284405e-03   1.96969292e-02  -5.70487470e-03   2.04980653e-05] 1.0 False\n",
      "[-0.00351891 -0.17534274 -0.00570446  0.29089802] 1.0 False\n",
      "[ -7.02576033e-03  -3.70382892e-01   1.13495692e-04   5.81776382e-01] 1.0 False\n",
      "[-0.01443342 -0.56550643  0.01174902  0.87449506] 1.0 False\n",
      "[-0.02574355 -0.37054617  0.02923892  0.58552898] 1.0 False\n",
      "[-0.03315447 -0.5660652   0.0409495   0.8872772 ] 1.0 False\n",
      "[-0.04447577 -0.76171834  0.05869505  1.19254666] 1.0 False\n",
      "[-0.05971014 -0.95754943  0.08254598  1.50303389] 1.0 False\n",
      "[-0.07886113 -0.76352073  0.11260666  1.2372228 ] 1.0 False\n",
      "[-0.09413154 -0.57001108  0.13735112  0.98183314] 1.0 False\n",
      "[-0.10553177 -0.76667962  0.15698778  1.31431203] 1.0 False\n",
      "[-0.12086536 -0.57385369  0.18327402  1.07459505] 1.0 False\n",
      "[-0.13234243 -0.77086147  0.20476592  1.418739  ] 1.0 False\n",
      "[-0.14775966 -0.96784343  0.2331407   1.76782046] 1.0 True\n",
      "Reward for this episode was: 15.0\n",
      "[-0.00796878 -0.22119862  0.01029628  0.2757435 ] 1.0 False\n",
      "[-0.01239276 -0.02622507  0.01581115 -0.01367427] 1.0 False\n",
      "[-0.01291726 -0.22157016  0.01553766  0.28395503] 1.0 False\n",
      "[-0.01734866 -0.41691024  0.02121676  0.58149766] 1.0 False\n",
      "[-0.02568687 -0.61232294  0.03284672  0.88078801] 1.0 False\n",
      "[-0.03793333 -0.41766223  0.05046248  0.59860964] 1.0 False\n",
      "[-0.04628657 -0.22328131  0.06243467  0.32223909] 1.0 False\n",
      "[-0.0507522  -0.02910143  0.06887945  0.04988065] 1.0 False\n",
      "[-0.05133422 -0.22513998  0.06987707  0.36347574] 1.0 False\n",
      "[-0.05583702 -0.42118188  0.07714658  0.67734811] 1.0 False\n",
      "[-0.06426066 -0.22721175  0.09069354  0.4099169 ] 1.0 False\n",
      "[-0.0688049  -0.4234946   0.09889188  0.7297585 ] 1.0 False\n",
      "[-0.07727479 -0.22986848  0.11348705  0.46976521] 1.0 False\n",
      "[-0.08187216 -0.03651721  0.12288235  0.21489759] 1.0 False\n",
      "[-0.0826025  -0.23316209  0.12718031  0.54367725] 1.0 False\n",
      "[-0.08726574 -0.42982031  0.13805385  0.87357241] 1.0 False\n",
      "[-0.09586215 -0.62652207  0.1555253   1.20627685] 1.0 False\n",
      "[-0.10839259 -0.82327336  0.17965084  1.54338203] 1.0 False\n",
      "[-0.12485806 -1.02004174  0.21051848  1.88631814] 1.0 True\n",
      "Reward for this episode was: 19.0\n",
      "[-0.02182386 -0.15359179 -0.04550103  0.3215429 ] 1.0 False\n",
      "[-0.0248957  -0.34803724 -0.03907017  0.59953652] 1.0 False\n",
      "[-0.03185644 -0.54259142 -0.02707944  0.87966137] 1.0 False\n",
      "[-0.04270827 -0.34711223 -0.00948621  0.57858969] 1.0 False\n",
      "[-0.04965052 -0.54209995  0.00208558  0.86826922] 1.0 False\n",
      "[-0.06049252 -0.34700644  0.01945097  0.57624275] 1.0 False\n",
      "[-0.06743264 -0.54239557  0.03097582  0.87498922] 1.0 False\n",
      "[-0.07828056 -0.3477081   0.04847561  0.59220355] 1.0 False\n",
      "[-0.08523472 -0.54347396  0.06031968  0.89975364] 1.0 False\n",
      "[-0.0961042  -0.73935919  0.07831475  1.21077033] 1.0 False\n",
      "[-0.11089138 -0.93540001  0.10253016  1.52693127] 1.0 False\n",
      "[-0.12959938 -0.74165391  0.13306878  1.26792941] 1.0 False\n",
      "[-0.14443246 -0.93820056  0.15842737  1.59914987] 1.0 False\n",
      "[-0.16319647 -0.74527062  0.19041037  1.35976221] 1.0 False\n",
      "[-0.17810188 -0.5529765   0.21760561  1.13217711] 1.0 True\n",
      "Reward for this episode was: 15.0\n",
      "[-0.01035483 -0.21873189  0.01845713  0.26701748] 1.0 False\n",
      "[-0.01472947 -0.41411233  0.02379748  0.56546431] 1.0 False\n",
      "[-0.02301171 -0.60955992  0.03510676  0.86554848] 1.0 False\n",
      "[-0.03520291 -0.41493293  0.05241773  0.58410715] 1.0 False\n",
      "[-0.04350157 -0.61074848  0.06409988  0.89283081] 1.0 False\n",
      "[-0.05571654 -0.41655179  0.08195649  0.62096646] 1.0 False\n",
      "[-0.06404758 -0.22266424  0.09437582  0.35517957] 1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06850086 -0.02900199  0.10147941  0.09368424] 1.0 False\n",
      "[-0.0690809  -0.22542097  0.1033531   0.41658073] 1.0 False\n",
      "[-0.07358932 -0.03190397  0.11168471  0.15818593] 1.0 False\n",
      "[-0.0742274  -0.22843301  0.11484843  0.48390994] 1.0 False\n",
      "[-0.07879606 -0.42497257  0.12452663  0.81046839] 1.0 False\n",
      "[-0.08729551 -0.62156051  0.140736    1.13958318] 1.0 False\n",
      "[-0.09972672 -0.81821343  0.16352766  1.47288604] 1.0 False\n",
      "[-0.11609099 -1.014913    0.19298538  1.81186232] 1.0 False\n",
      "[-0.13638925 -0.82239443  0.22922263  1.58483149] 1.0 True\n",
      "Reward for this episode was: 16.0\n",
      "[ 0.0197605  -0.23300508  0.03854025  0.3518877 ] 1.0 False\n",
      "[ 0.01510039 -0.4286533   0.04557801  0.65647013] 1.0 False\n",
      "[ 0.00652733 -0.62437912  0.05870741  0.96314926] 1.0 False\n",
      "[-0.00596025 -0.43009302  0.07797039  0.68947225] 1.0 False\n",
      "[-0.01456211 -0.23613473  0.09175984  0.42231881] 1.0 False\n",
      "[-0.01928481 -0.43242874  0.10020622  0.74246066] 1.0 False\n",
      "[-0.02793338 -0.23882226  0.11505543  0.48291912] 1.0 False\n",
      "[-0.03270983 -0.4353641   0.12471381  0.80953441] 1.0 False\n",
      "[-0.04141711 -0.24215146  0.1409045   0.55853727] 1.0 False\n",
      "[-0.04626014 -0.43894081  0.15207525  0.89208481] 1.0 False\n",
      "[-0.05503896 -0.24617245  0.16991694  0.65080742] 1.0 False\n",
      "[-0.05996241 -0.0537734   0.18293309  0.41608056] 1.0 False\n",
      "[-0.06103787 -0.25095256  0.1912547   0.76039707] 1.0 False\n",
      "[-0.06605693 -0.05890762  0.20646264  0.53346884] 1.0 False\n",
      "[-0.06723508 -0.2562434   0.21713202  0.88345574] 1.0 True\n",
      "Reward for this episode was: 15.0\n",
      "[ 0.02510426  0.19286155  0.04399727 -0.23306881] 1.0 False\n",
      "[ 0.02896149 -0.00286054  0.0393359   0.07316124] 1.0 False\n",
      "[ 0.02890428 -0.19852369  0.04079912  0.37799071] 1.0 False\n",
      "[ 0.0249338  -0.00400421  0.04835894  0.0984459 ] 1.0 False\n",
      "[ 0.02485372 -0.19978468  0.05032786  0.40598528] 1.0 False\n",
      "[ 0.02085802 -0.39558284  0.05844756  0.71410079] 1.0 False\n",
      "[ 0.01294637 -0.59146314  0.07272958  1.02459333] 1.0 False\n",
      "[ 0.0011171  -0.39738113  0.09322144  0.75560358] 1.0 False\n",
      "[-0.00683052 -0.59365598  0.10833351  1.07610496] 1.0 False\n",
      "[-0.01870364 -0.40011896  0.12985561  0.81928909] 1.0 False\n",
      "[-0.02670602 -0.59675631  0.1462414   1.1498321 ] 1.0 False\n",
      "[-0.03864114 -0.79345223  0.16923804  1.48456849] 1.0 False\n",
      "[-0.05451019 -0.99018469  0.19892941  1.82497003] 1.0 False\n",
      "[-0.07431388 -0.79774878  0.23542881  1.60010907] 1.0 True\n",
      "Reward for this episode was: 14.0\n",
      "[-0.02361855 -0.23233665 -0.00235074  0.26744167] 1.0 False\n",
      "[-0.02826528 -0.42742497  0.00299809  0.55938223] 1.0 False\n",
      "[-0.03681378 -0.23234523  0.01418573  0.26764537] 1.0 False\n",
      "[-0.04146069 -0.03742857  0.01953864 -0.02052973] 1.0 False\n",
      "[-0.04220926  0.1574078   0.01912805 -0.30698452] 1.0 False\n",
      "[-0.0390611  -0.03798142  0.01298836 -0.008331  ] 1.0 False\n",
      "[-0.03982073  0.15695188  0.01282174 -0.29688781] 1.0 False\n",
      "[-0.03668169 -0.03835049  0.00688398 -0.00018886] 1.0 False\n",
      "[-0.0374487   0.15667207  0.0068802  -0.29069188] 1.0 False\n",
      "[-0.03431526  0.35169524  0.00106636 -0.58119696] 1.0 False\n",
      "[-0.02728136  0.54680223 -0.01055757 -0.87354377] 1.0 False\n",
      "[-0.01634531  0.74206613 -0.02802845 -1.16952716] 1.0 False\n",
      "[-0.00150399  0.93754121 -0.05141899 -1.4708639 ] 1.0 False\n",
      "[ 0.01724684  0.74308449 -0.08083627 -1.19467485] 1.0 False\n",
      "[ 0.03210853  0.54909692 -0.10472977 -0.92838368] 1.0 False\n",
      "[ 0.04309046  0.74546496 -0.12329744 -1.25205614] 1.0 False\n",
      "[ 0.05799976  0.5521193  -0.14833856 -1.000397  ] 1.0 False\n",
      "[ 0.06904215  0.74887879 -0.1683465  -1.33574678] 1.0 False\n",
      "[ 0.08401972  0.55622989 -0.19506144 -1.10011903] 1.0 False\n",
      "[ 0.09514432  0.36413418 -0.21706382 -0.87442497] 1.0 True\n",
      "Reward for this episode was: 20.0\n",
      "[-0.0018446   0.23771644 -0.00920716 -0.33581413] 1.0 False\n",
      "[ 0.00290973  0.4329682  -0.01592344 -0.63138623] 1.0 False\n",
      "[ 0.0115691   0.238072   -0.02855117 -0.34376029] 1.0 False\n",
      "[ 0.01633054  0.04336761 -0.03542637 -0.06021564] 1.0 False\n",
      "[ 0.01719789 -0.15122896 -0.03663069  0.2210829 ] 1.0 False\n",
      "[ 0.01417331  0.04439691 -0.03220903 -0.08292608] 1.0 False\n",
      "[ 0.01506125  0.23996541 -0.03386755 -0.38559449] 1.0 False\n",
      "[ 0.01986056  0.04534021 -0.04157944 -0.10377927] 1.0 False\n",
      "[ 0.02076736 -0.14916196 -0.04365503  0.175501  ] 1.0 False\n",
      "[ 0.01778412 -0.34363284 -0.04014501  0.4540989 ] 1.0 False\n",
      "[ 0.01091146 -0.53816482 -0.03106303  0.7338623 ] 1.0 False\n",
      "[  1.48167458e-04  -7.32844144e-01  -1.63857818e-02   1.01660935e+00] 1.0 False\n",
      "[-0.01450872 -0.92774383  0.00394641  1.30410244] 1.0 False\n",
      "[-0.03306359 -0.73267215  0.03002845  1.01265744] 1.0 False\n",
      "[-0.04771704 -0.5379634   0.0502816   0.72955301] 1.0 False\n",
      "[-0.0584763  -0.34357118  0.06487266  0.45310977] 1.0 False\n",
      "[-0.06534773 -0.1494237   0.07393486  0.18156033] 1.0 False\n",
      "[-0.0683362   0.0445667   0.07756606 -0.08691327] 1.0 False\n",
      "[-0.06744487  0.23849606  0.0758278  -0.35415111] 1.0 False\n",
      "[-0.06267495  0.43246252  0.06874478 -0.62199272] 1.0 False\n",
      "[-0.0540257   0.62656058  0.05630492 -0.89225709] 1.0 False\n",
      "[-0.04149448  0.43072193  0.03845978 -0.58241973] 1.0 False\n",
      "[-0.03288004  0.23508285  0.02681139 -0.27787371] 1.0 False\n",
      "[-0.02817839  0.03958886  0.02125391  0.02314347] 1.0 False\n",
      "[-0.02738661 -0.15583134  0.02171678  0.32245575] 1.0 False\n",
      "[-0.03050324 -0.3512557   0.0281659   0.62190739] 1.0 False\n",
      "[-0.03752835 -0.5467594   0.04060404  0.92332632] 1.0 False\n",
      "[-0.04846354 -0.74240568  0.05907057  1.22848816] 1.0 False\n",
      "[-0.06331165 -0.54809155  0.08364033  0.95488199] 1.0 False\n",
      "[-0.07427348 -0.74423296  0.10273797  1.27262718] 1.0 False\n",
      "[-0.08915814 -0.55056108  0.12819052  1.01380304] 1.0 False\n",
      "[-0.10016936 -0.74713813  0.14846658  1.34383408] 1.0 False\n",
      "[-0.11511213 -0.5541626   0.17534326  1.10104415] 1.0 False\n",
      "[-0.12619538 -0.75110316  0.19736414  1.44321253] 1.0 False\n",
      "[-0.14121744 -0.5588811   0.22622839  1.21812598] 1.0 True\n",
      "Reward for this episode was: 35.0\n",
      "[-0.03749771  0.2198342  -0.03888226 -0.31252099] 1.0 False\n",
      "[-0.03310103  0.02528712 -0.04513268 -0.03234944] 1.0 False\n",
      "[-0.03259529  0.22102626 -0.04577967 -0.33892367] 1.0 False\n",
      "[-0.02817476  0.41676872 -0.05255814 -0.64568426] 1.0 False\n",
      "[-0.01983939  0.22241702 -0.06547183 -0.37000425] 1.0 False\n",
      "[-0.01539105  0.41840513 -0.07287191 -0.68259152] 1.0 False\n",
      "[-0.00702294  0.22436674 -0.08652374 -0.41371178] 1.0 False\n",
      "[-0.00253561  0.03057094 -0.09479798 -0.14951178] 1.0 False\n",
      "[-0.00192419 -0.16307471 -0.09778821  0.11182362] 1.0 False\n",
      "[-0.00518568  0.03330258 -0.09555174 -0.21003898] 1.0 False\n",
      "[-0.00451963  0.22965168 -0.09975252 -0.53126861] 1.0 False\n",
      "[  7.34018669e-05   3.60639162e-02  -1.10377892e-01  -2.71609123e-01] 1.0 False\n",
      "[ 0.00079468 -0.15732411 -0.11581007 -0.01567761] 1.0 False\n",
      "[-0.0023518   0.03925184 -0.11612363 -0.34253849] 1.0 False\n",
      "[-0.00156677 -0.15404306 -0.1229744  -0.08861251] 1.0 False\n",
      "[-0.00464763 -0.34720738 -0.12474665  0.16288243] 1.0 False\n",
      "[-0.01159177 -0.15054088 -0.121489   -0.16640541] 1.0 False\n",
      "[-0.01460259 -0.34373325 -0.12481711  0.08561723] 1.0 False\n",
      "[-0.02147726 -0.14706369 -0.12310476 -0.24369312] 1.0 False\n",
      "[-0.02441853  0.04958191 -0.12797862 -0.5725307 ] 1.0 False\n",
      "[-0.02342689 -0.14353526 -0.13942924 -0.32274704] 1.0 False\n",
      "[-0.0262976  -0.33642492 -0.14588418 -0.0770799 ] 1.0 False\n",
      "[-0.0330261  -0.1395456  -0.14742578 -0.41199992] 1.0 False\n",
      "[-0.03581701  0.057325   -0.15566578 -0.74728883] 1.0 False\n",
      "[-0.03467051  0.25421279 -0.17061155 -1.08463064] 1.0 False\n",
      "[-0.02958625  0.06170164 -0.19230416 -0.8499733 ] 1.0 False\n",
      "[-0.02835222 -0.13035128 -0.20930363 -0.62339384] 1.0 False\n",
      "[-0.03095924  0.06698334 -0.22177151 -0.97402274] 1.0 True\n",
      "Reward for this episode was: 28.0\n",
      "[-0.04574729 -0.20283551 -0.02259417  0.28108097] 1.0 False\n",
      "[-0.049804   -0.39762801 -0.01697255  0.56655301] 1.0 False\n",
      "[-0.05775656 -0.20227212 -0.00564149  0.2685717 ] 1.0 False\n",
      "[-0.06180201 -0.00707012 -0.00027006 -0.02588524] 1.0 False\n",
      "[-0.06194341 -0.2021882  -0.00078776  0.26671247] 1.0 False\n",
      "[-0.06598717 -0.3972989   0.00454649  0.55914683] 1.0 False\n",
      "[-0.07393315 -0.59248437  0.01572943  0.85325867] 1.0 False\n",
      "[-0.08578284 -0.78781716  0.0327946   1.15084587] 1.0 False\n",
      "[-0.10153918 -0.59313814  0.05581152  0.86862424] 1.0 False\n",
      "[-0.11340194 -0.39881813  0.073184    0.59399815] 1.0 False\n",
      "[-0.12137831 -0.20479273  0.08506396  0.32523598] 1.0 False\n",
      "[-0.12547416 -0.01097844  0.09156868  0.06054439] 1.0 False\n",
      "[-0.12569373 -0.20728587  0.09277957  0.38065547] 1.0 False\n",
      "[-0.12983945 -0.4035944   0.10039268  0.70108988] 1.0 False\n",
      "[-0.13791134 -0.2099968   0.11441448  0.44162156] 1.0 False\n",
      "[-0.14211127 -0.40653618  0.12324691  0.76806762] 1.0 False\n",
      "[-0.150242   -0.60311969  0.13860826  1.0968491 ] 1.0 False\n",
      "[-0.16230439 -0.41006771  0.16054524  0.85066886] 1.0 False\n",
      "[-0.17050574 -0.60697172  0.17755862  1.18922448] 1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.18264518 -0.80389372  0.20134311  1.53189179] 1.0 False\n",
      "[-0.19872305 -1.00079175  0.23198095  1.88006823] 1.0 True\n",
      "Reward for this episode was: 21.0\n",
      "[ 0.0250609   0.1727176  -0.0262518  -0.31390668] 1.0 False\n",
      "[ 0.02851525 -0.02202074 -0.03252993 -0.02961696] 1.0 False\n",
      "[ 0.02807484  0.17355224 -0.03312227 -0.33238335] 1.0 False\n",
      "[ 0.03154588  0.3691296  -0.03976994 -0.63532465] 1.0 False\n",
      "[ 0.03892847  0.17458425 -0.05247643 -0.35542665] 1.0 False\n",
      "[ 0.04242016  0.37041153 -0.05958496 -0.6641842 ] 1.0 False\n",
      "[ 0.04982839  0.56630953 -0.07286865 -0.97501734] 1.0 False\n",
      "[ 0.06115458  0.37223667 -0.09236899 -0.70608501] 1.0 False\n",
      "[ 0.06859931  0.17850764 -0.1064907  -0.44384815] 1.0 False\n",
      "[ 0.07216947 -0.0149591  -0.11536766 -0.18654108] 1.0 False\n",
      "[ 0.07187028 -0.20825764 -0.11909848  0.06763641] 1.0 False\n",
      "[ 0.06770513 -0.40148858 -0.11774575  0.32049736] 1.0 False\n",
      "[ 0.05967536 -0.20490382 -0.1113358  -0.00687536] 1.0 False\n",
      "[ 0.05557728 -0.00837583 -0.11147331 -0.3325073 ] 1.0 False\n",
      "[ 0.05540977  0.18814166 -0.11812346 -0.65815926] 1.0 False\n",
      "[ 0.0591726  -0.00515532 -0.13128664 -0.40488187] 1.0 False\n",
      "[ 0.05906949  0.19156031 -0.13938428 -0.73590347] 1.0 False\n",
      "[ 0.0629007   0.38830416 -0.15410235 -1.06900356] 1.0 False\n",
      "[ 0.07066678  0.19551866 -0.17548242 -0.82837913] 1.0 False\n",
      "[ 0.07457716  0.39254964 -0.19205    -1.17071418] 1.0 False\n",
      "[ 0.08242815  0.20037181 -0.21546429 -0.94386348] 1.0 True\n",
      "Reward for this episode was: 21.0\n"
     ]
    }
   ],
   "source": [
    "while random_episodes < 50:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    print(observation, reward, done)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(\"Reward for this episode was:\", reward_sum)\n",
    "        reward_sum = 0\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1, reward, done, _ = env.step(a)\n",
    "if done:\n",
    "    Qs[0, a] = -100\n",
    "else:\n",
    "    x1 = np.reshape(s1, [1, input_size])\n",
    "    Qs1 = sess.run(Qpred, feed_dict={X: x1})\n",
    "    Qs[0, a] = reward + dis * np.max(Qs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Qs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c6e71cd48007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdis\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Qs1' is not defined"
     ]
    }
   ],
   "source": [
    "Qs[0,a] = reward + dis * np.max(Qs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-add57766a6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode: {} steps: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'step_count' is not defined"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    step_count += 1\n",
    "    x = np.reshape(s, [1, input_size])\n",
    "    Qs = sess.run(Qpred, feed_dict={X: x})\n",
    "    if np.random.rand(1) < e:\n",
    "        a = env.action_space.sample()\n",
    "    else:\n",
    "        np.argmax(Qs)\n",
    "    s1, reward, done, _ = env.step(a)\n",
    "    if done:\n",
    "        Qs[0, a] = -100\n",
    "    else:\n",
    "        x1 = np.reshape(s1, [1, input_size])\n",
    "        Qs1 = sess.run(Qpred, feed_dict={X: x1})\n",
    "        Qs[0, a] = reward + dis * np.max(Qs1)\n",
    "    sess.run(train, feed_dict={X: x, Y: Qs})\n",
    "    s = s1\n",
    "rList.append(step_count)\n",
    "print(\"Episode: {} steps: {}\".format(i, step_count))\n",
    "if len(rList) > 10 and np.mean(rList[-10:]) > 500:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e088a5a8ad57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mQs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    env.render()\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    Qs = sess.run(Qpred, feed_dict={X: x})\n",
    "    a = np.argmax(Qs)\n",
    "    \n",
    "    observation, reward, done, _ = env.step(a)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total Score: {}\".format(reward_sum))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-28 15:33:31,747] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis = .99\n",
    "num_episodes = 2000\n",
    "learning_rate = 0.1\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "Qpred = tf.matmul(X, W1)\n",
    "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(Y-Qpred))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 steps: 13\n",
      "Episode: 1 steps: 22\n",
      "Episode: 2 steps: 13\n",
      "Episode: 3 steps: 10\n",
      "Episode: 4 steps: 12\n",
      "Episode: 5 steps: 42\n",
      "Episode: 6 steps: 9\n",
      "Episode: 7 steps: 40\n",
      "Episode: 8 steps: 15\n",
      "Episode: 9 steps: 11\n",
      "Episode: 10 steps: 17\n",
      "Episode: 11 steps: 41\n",
      "Episode: 12 steps: 10\n",
      "Episode: 13 steps: 12\n",
      "Episode: 14 steps: 49\n",
      "Episode: 15 steps: 26\n",
      "Episode: 16 steps: 17\n",
      "Episode: 17 steps: 8\n",
      "Episode: 18 steps: 9\n",
      "Episode: 19 steps: 10\n",
      "Episode: 20 steps: 10\n",
      "Episode: 21 steps: 9\n",
      "Episode: 22 steps: 8\n",
      "Episode: 23 steps: 9\n",
      "Episode: 24 steps: 11\n",
      "Episode: 25 steps: 26\n",
      "Episode: 26 steps: 22\n",
      "Episode: 27 steps: 26\n",
      "Episode: 28 steps: 11\n",
      "Episode: 29 steps: 9\n",
      "Episode: 30 steps: 10\n",
      "Episode: 31 steps: 12\n",
      "Episode: 32 steps: 10\n",
      "Episode: 33 steps: 10\n",
      "Episode: 34 steps: 22\n",
      "Episode: 35 steps: 8\n",
      "Episode: 36 steps: 13\n",
      "Episode: 37 steps: 9\n",
      "Episode: 38 steps: 9\n",
      "Episode: 39 steps: 9\n",
      "Episode: 40 steps: 17\n",
      "Episode: 41 steps: 10\n",
      "Episode: 42 steps: 9\n",
      "Episode: 43 steps: 16\n",
      "Episode: 44 steps: 9\n",
      "Episode: 45 steps: 9\n",
      "Episode: 46 steps: 10\n",
      "Episode: 47 steps: 10\n",
      "Episode: 48 steps: 10\n",
      "Episode: 49 steps: 8\n",
      "Episode: 50 steps: 10\n",
      "Episode: 51 steps: 27\n",
      "Episode: 52 steps: 10\n",
      "Episode: 53 steps: 10\n",
      "Episode: 54 steps: 12\n",
      "Episode: 55 steps: 22\n",
      "Episode: 56 steps: 9\n",
      "Episode: 57 steps: 9\n",
      "Episode: 58 steps: 9\n",
      "Episode: 59 steps: 9\n",
      "Episode: 60 steps: 9\n",
      "Episode: 61 steps: 14\n",
      "Episode: 62 steps: 8\n",
      "Episode: 63 steps: 9\n",
      "Episode: 64 steps: 10\n",
      "Episode: 65 steps: 14\n",
      "Episode: 66 steps: 9\n",
      "Episode: 67 steps: 9\n",
      "Episode: 68 steps: 10\n",
      "Episode: 69 steps: 10\n",
      "Episode: 70 steps: 10\n",
      "Episode: 71 steps: 9\n",
      "Episode: 72 steps: 10\n",
      "Episode: 73 steps: 9\n",
      "Episode: 74 steps: 9\n",
      "Episode: 75 steps: 10\n",
      "Episode: 76 steps: 9\n",
      "Episode: 77 steps: 10\n",
      "Episode: 78 steps: 12\n",
      "Episode: 79 steps: 9\n",
      "Episode: 80 steps: 9\n",
      "Episode: 81 steps: 11\n",
      "Episode: 82 steps: 10\n",
      "Episode: 83 steps: 9\n",
      "Episode: 84 steps: 10\n",
      "Episode: 85 steps: 10\n",
      "Episode: 86 steps: 24\n",
      "Episode: 87 steps: 11\n",
      "Episode: 88 steps: 9\n",
      "Episode: 89 steps: 10\n",
      "Episode: 90 steps: 8\n",
      "Episode: 91 steps: 10\n",
      "Episode: 92 steps: 9\n",
      "Episode: 93 steps: 9\n",
      "Episode: 94 steps: 10\n",
      "Episode: 95 steps: 10\n",
      "Episode: 96 steps: 9\n",
      "Episode: 97 steps: 9\n",
      "Episode: 98 steps: 10\n",
      "Episode: 99 steps: 10\n",
      "Episode: 100 steps: 8\n",
      "Episode: 101 steps: 8\n",
      "Episode: 102 steps: 21\n",
      "Episode: 103 steps: 8\n",
      "Episode: 104 steps: 17\n",
      "Episode: 105 steps: 11\n",
      "Episode: 106 steps: 9\n",
      "Episode: 107 steps: 10\n",
      "Episode: 108 steps: 8\n",
      "Episode: 109 steps: 10\n",
      "Episode: 110 steps: 9\n",
      "Episode: 111 steps: 9\n",
      "Episode: 112 steps: 10\n",
      "Episode: 113 steps: 10\n",
      "Episode: 114 steps: 10\n",
      "Episode: 115 steps: 10\n",
      "Episode: 116 steps: 13\n",
      "Episode: 117 steps: 9\n",
      "Episode: 118 steps: 17\n",
      "Episode: 119 steps: 10\n",
      "Episode: 120 steps: 10\n",
      "Episode: 121 steps: 10\n",
      "Episode: 122 steps: 9\n",
      "Episode: 123 steps: 11\n",
      "Episode: 124 steps: 9\n",
      "Episode: 125 steps: 9\n",
      "Episode: 126 steps: 10\n",
      "Episode: 127 steps: 10\n",
      "Episode: 128 steps: 8\n",
      "Episode: 129 steps: 10\n",
      "Episode: 130 steps: 10\n",
      "Episode: 131 steps: 10\n",
      "Episode: 132 steps: 40\n",
      "Episode: 133 steps: 9\n",
      "Episode: 134 steps: 10\n",
      "Episode: 135 steps: 10\n",
      "Episode: 136 steps: 10\n",
      "Episode: 137 steps: 8\n",
      "Episode: 138 steps: 9\n",
      "Episode: 139 steps: 12\n",
      "Episode: 140 steps: 11\n",
      "Episode: 141 steps: 9\n",
      "Episode: 142 steps: 10\n",
      "Episode: 143 steps: 9\n",
      "Episode: 144 steps: 9\n",
      "Episode: 145 steps: 8\n",
      "Episode: 146 steps: 10\n",
      "Episode: 147 steps: 10\n",
      "Episode: 148 steps: 10\n",
      "Episode: 149 steps: 12\n",
      "Episode: 150 steps: 8\n",
      "Episode: 151 steps: 9\n",
      "Episode: 152 steps: 8\n",
      "Episode: 153 steps: 12\n",
      "Episode: 154 steps: 9\n",
      "Episode: 155 steps: 9\n",
      "Episode: 156 steps: 10\n",
      "Episode: 157 steps: 25\n",
      "Episode: 158 steps: 10\n",
      "Episode: 159 steps: 9\n",
      "Episode: 160 steps: 8\n",
      "Episode: 161 steps: 9\n",
      "Episode: 162 steps: 32\n",
      "Episode: 163 steps: 8\n",
      "Episode: 164 steps: 21\n",
      "Episode: 165 steps: 10\n",
      "Episode: 166 steps: 9\n",
      "Episode: 167 steps: 10\n",
      "Episode: 168 steps: 9\n",
      "Episode: 169 steps: 10\n",
      "Episode: 170 steps: 9\n",
      "Episode: 171 steps: 10\n",
      "Episode: 172 steps: 10\n",
      "Episode: 173 steps: 25\n",
      "Episode: 174 steps: 9\n",
      "Episode: 175 steps: 12\n",
      "Episode: 176 steps: 10\n",
      "Episode: 177 steps: 9\n",
      "Episode: 178 steps: 9\n",
      "Episode: 179 steps: 10\n",
      "Episode: 180 steps: 10\n",
      "Episode: 181 steps: 10\n",
      "Episode: 182 steps: 9\n",
      "Episode: 183 steps: 9\n",
      "Episode: 184 steps: 10\n",
      "Episode: 185 steps: 10\n",
      "Episode: 186 steps: 9\n",
      "Episode: 187 steps: 10\n",
      "Episode: 188 steps: 10\n",
      "Episode: 189 steps: 9\n",
      "Episode: 190 steps: 10\n",
      "Episode: 191 steps: 10\n",
      "Episode: 192 steps: 10\n",
      "Episode: 193 steps: 9\n",
      "Episode: 194 steps: 9\n",
      "Episode: 195 steps: 9\n",
      "Episode: 196 steps: 10\n",
      "Episode: 197 steps: 18\n",
      "Episode: 198 steps: 10\n",
      "Episode: 199 steps: 9\n",
      "Episode: 200 steps: 8\n",
      "Episode: 201 steps: 9\n",
      "Episode: 202 steps: 10\n",
      "Episode: 203 steps: 9\n",
      "Episode: 204 steps: 12\n",
      "Episode: 205 steps: 10\n",
      "Episode: 206 steps: 9\n",
      "Episode: 207 steps: 17\n",
      "Episode: 208 steps: 11\n",
      "Episode: 209 steps: 9\n",
      "Episode: 210 steps: 10\n",
      "Episode: 211 steps: 8\n",
      "Episode: 212 steps: 10\n",
      "Episode: 213 steps: 24\n",
      "Episode: 214 steps: 10\n",
      "Episode: 215 steps: 11\n",
      "Episode: 216 steps: 10\n",
      "Episode: 217 steps: 9\n",
      "Episode: 218 steps: 9\n",
      "Episode: 219 steps: 11\n",
      "Episode: 220 steps: 9\n",
      "Episode: 221 steps: 9\n",
      "Episode: 222 steps: 9\n",
      "Episode: 223 steps: 8\n",
      "Episode: 224 steps: 9\n",
      "Episode: 225 steps: 9\n",
      "Episode: 226 steps: 11\n",
      "Episode: 227 steps: 9\n",
      "Episode: 228 steps: 10\n",
      "Episode: 229 steps: 10\n",
      "Episode: 230 steps: 9\n",
      "Episode: 231 steps: 10\n",
      "Episode: 232 steps: 10\n",
      "Episode: 233 steps: 10\n",
      "Episode: 234 steps: 19\n",
      "Episode: 235 steps: 10\n",
      "Episode: 236 steps: 23\n",
      "Episode: 237 steps: 10\n",
      "Episode: 238 steps: 9\n",
      "Episode: 239 steps: 10\n",
      "Episode: 240 steps: 9\n",
      "Episode: 241 steps: 10\n",
      "Episode: 242 steps: 14\n",
      "Episode: 243 steps: 8\n",
      "Episode: 244 steps: 10\n",
      "Episode: 245 steps: 9\n",
      "Episode: 246 steps: 10\n",
      "Episode: 247 steps: 9\n",
      "Episode: 248 steps: 9\n",
      "Episode: 249 steps: 9\n",
      "Episode: 250 steps: 11\n",
      "Episode: 251 steps: 9\n",
      "Episode: 252 steps: 11\n",
      "Episode: 253 steps: 9\n",
      "Episode: 254 steps: 10\n",
      "Episode: 255 steps: 10\n",
      "Episode: 256 steps: 8\n",
      "Episode: 257 steps: 9\n",
      "Episode: 258 steps: 9\n",
      "Episode: 259 steps: 10\n",
      "Episode: 260 steps: 9\n",
      "Episode: 261 steps: 9\n",
      "Episode: 262 steps: 8\n",
      "Episode: 263 steps: 10\n",
      "Episode: 264 steps: 9\n",
      "Episode: 265 steps: 9\n",
      "Episode: 266 steps: 10\n",
      "Episode: 267 steps: 10\n",
      "Episode: 268 steps: 9\n",
      "Episode: 269 steps: 9\n",
      "Episode: 270 steps: 9\n",
      "Episode: 271 steps: 8\n",
      "Episode: 272 steps: 9\n",
      "Episode: 273 steps: 9\n",
      "Episode: 274 steps: 9\n",
      "Episode: 275 steps: 9\n",
      "Episode: 276 steps: 9\n",
      "Episode: 277 steps: 9\n",
      "Episode: 278 steps: 17\n",
      "Episode: 279 steps: 10\n",
      "Episode: 280 steps: 10\n",
      "Episode: 281 steps: 11\n",
      "Episode: 282 steps: 9\n",
      "Episode: 283 steps: 9\n",
      "Episode: 284 steps: 10\n",
      "Episode: 285 steps: 9\n",
      "Episode: 286 steps: 9\n",
      "Episode: 287 steps: 8\n",
      "Episode: 288 steps: 9\n",
      "Episode: 289 steps: 10\n",
      "Episode: 290 steps: 9\n",
      "Episode: 291 steps: 9\n",
      "Episode: 292 steps: 10\n",
      "Episode: 293 steps: 10\n",
      "Episode: 294 steps: 9\n",
      "Episode: 295 steps: 9\n",
      "Episode: 296 steps: 10\n",
      "Episode: 297 steps: 9\n",
      "Episode: 298 steps: 10\n",
      "Episode: 299 steps: 9\n",
      "Episode: 300 steps: 9\n",
      "Episode: 301 steps: 9\n",
      "Episode: 302 steps: 10\n",
      "Episode: 303 steps: 11\n",
      "Episode: 304 steps: 10\n",
      "Episode: 305 steps: 9\n",
      "Episode: 306 steps: 11\n",
      "Episode: 307 steps: 9\n",
      "Episode: 308 steps: 10\n",
      "Episode: 309 steps: 11\n",
      "Episode: 310 steps: 22\n",
      "Episode: 311 steps: 9\n",
      "Episode: 312 steps: 9\n",
      "Episode: 313 steps: 10\n",
      "Episode: 314 steps: 10\n",
      "Episode: 315 steps: 10\n",
      "Episode: 316 steps: 11\n",
      "Episode: 317 steps: 9\n",
      "Episode: 318 steps: 10\n",
      "Episode: 319 steps: 10\n",
      "Episode: 320 steps: 18\n",
      "Episode: 321 steps: 10\n",
      "Episode: 322 steps: 10\n",
      "Episode: 323 steps: 9\n",
      "Episode: 324 steps: 15\n",
      "Episode: 325 steps: 9\n",
      "Episode: 326 steps: 9\n",
      "Episode: 327 steps: 10\n",
      "Episode: 328 steps: 9\n",
      "Episode: 329 steps: 10\n",
      "Episode: 330 steps: 9\n",
      "Episode: 331 steps: 10\n",
      "Episode: 332 steps: 8\n",
      "Episode: 333 steps: 10\n",
      "Episode: 334 steps: 9\n",
      "Episode: 335 steps: 10\n",
      "Episode: 336 steps: 10\n",
      "Episode: 337 steps: 10\n",
      "Episode: 338 steps: 9\n",
      "Episode: 339 steps: 9\n",
      "Episode: 340 steps: 8\n",
      "Episode: 341 steps: 10\n",
      "Episode: 342 steps: 10\n",
      "Episode: 343 steps: 10\n",
      "Episode: 344 steps: 10\n",
      "Episode: 345 steps: 8\n",
      "Episode: 346 steps: 9\n",
      "Episode: 347 steps: 10\n",
      "Episode: 348 steps: 8\n",
      "Episode: 349 steps: 10\n",
      "Episode: 350 steps: 10\n",
      "Episode: 351 steps: 9\n",
      "Episode: 352 steps: 9\n",
      "Episode: 353 steps: 9\n",
      "Episode: 354 steps: 10\n",
      "Episode: 355 steps: 9\n",
      "Episode: 356 steps: 9\n",
      "Episode: 357 steps: 11\n",
      "Episode: 358 steps: 9\n",
      "Episode: 359 steps: 10\n",
      "Episode: 360 steps: 8\n",
      "Episode: 361 steps: 11\n",
      "Episode: 362 steps: 11\n",
      "Episode: 363 steps: 10\n",
      "Episode: 364 steps: 9\n",
      "Episode: 365 steps: 10\n",
      "Episode: 366 steps: 8\n",
      "Episode: 367 steps: 9\n",
      "Episode: 368 steps: 10\n",
      "Episode: 369 steps: 10\n",
      "Episode: 370 steps: 10\n",
      "Episode: 371 steps: 9\n",
      "Episode: 372 steps: 9\n",
      "Episode: 373 steps: 13\n",
      "Episode: 374 steps: 10\n",
      "Episode: 375 steps: 10\n",
      "Episode: 376 steps: 9\n",
      "Episode: 377 steps: 9\n",
      "Episode: 378 steps: 10\n",
      "Episode: 379 steps: 12\n",
      "Episode: 380 steps: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 381 steps: 22\n",
      "Episode: 382 steps: 10\n",
      "Episode: 383 steps: 9\n",
      "Episode: 384 steps: 9\n",
      "Episode: 385 steps: 10\n",
      "Episode: 386 steps: 8\n",
      "Episode: 387 steps: 19\n",
      "Episode: 388 steps: 10\n",
      "Episode: 389 steps: 9\n",
      "Episode: 390 steps: 8\n",
      "Episode: 391 steps: 10\n",
      "Episode: 392 steps: 10\n",
      "Episode: 393 steps: 9\n",
      "Episode: 394 steps: 8\n",
      "Episode: 395 steps: 10\n",
      "Episode: 396 steps: 9\n",
      "Episode: 397 steps: 8\n",
      "Episode: 398 steps: 10\n",
      "Episode: 399 steps: 8\n",
      "Episode: 400 steps: 9\n",
      "Episode: 401 steps: 9\n",
      "Episode: 402 steps: 8\n",
      "Episode: 403 steps: 10\n",
      "Episode: 404 steps: 10\n",
      "Episode: 405 steps: 9\n",
      "Episode: 406 steps: 10\n",
      "Episode: 407 steps: 8\n",
      "Episode: 408 steps: 9\n",
      "Episode: 409 steps: 10\n",
      "Episode: 410 steps: 10\n",
      "Episode: 411 steps: 10\n",
      "Episode: 412 steps: 10\n",
      "Episode: 413 steps: 11\n",
      "Episode: 414 steps: 10\n",
      "Episode: 415 steps: 9\n",
      "Episode: 416 steps: 10\n",
      "Episode: 417 steps: 8\n",
      "Episode: 418 steps: 11\n",
      "Episode: 419 steps: 8\n",
      "Episode: 420 steps: 9\n",
      "Episode: 421 steps: 10\n",
      "Episode: 422 steps: 9\n",
      "Episode: 423 steps: 9\n",
      "Episode: 424 steps: 10\n",
      "Episode: 425 steps: 9\n",
      "Episode: 426 steps: 9\n",
      "Episode: 427 steps: 9\n",
      "Episode: 428 steps: 10\n",
      "Episode: 429 steps: 10\n",
      "Episode: 430 steps: 9\n",
      "Episode: 431 steps: 10\n",
      "Episode: 432 steps: 10\n",
      "Episode: 433 steps: 9\n",
      "Episode: 434 steps: 10\n",
      "Episode: 435 steps: 10\n",
      "Episode: 436 steps: 9\n",
      "Episode: 437 steps: 8\n",
      "Episode: 438 steps: 10\n",
      "Episode: 439 steps: 10\n",
      "Episode: 440 steps: 10\n",
      "Episode: 441 steps: 9\n",
      "Episode: 442 steps: 10\n",
      "Episode: 443 steps: 9\n",
      "Episode: 444 steps: 8\n",
      "Episode: 445 steps: 9\n",
      "Episode: 446 steps: 11\n",
      "Episode: 447 steps: 10\n",
      "Episode: 448 steps: 9\n",
      "Episode: 449 steps: 8\n",
      "Episode: 450 steps: 9\n",
      "Episode: 451 steps: 8\n",
      "Episode: 452 steps: 10\n",
      "Episode: 453 steps: 9\n",
      "Episode: 454 steps: 9\n",
      "Episode: 455 steps: 10\n",
      "Episode: 456 steps: 10\n",
      "Episode: 457 steps: 9\n",
      "Episode: 458 steps: 9\n",
      "Episode: 459 steps: 10\n",
      "Episode: 460 steps: 10\n",
      "Episode: 461 steps: 10\n",
      "Episode: 462 steps: 10\n",
      "Episode: 463 steps: 9\n",
      "Episode: 464 steps: 8\n",
      "Episode: 465 steps: 10\n",
      "Episode: 466 steps: 8\n",
      "Episode: 467 steps: 10\n",
      "Episode: 468 steps: 14\n",
      "Episode: 469 steps: 9\n",
      "Episode: 470 steps: 9\n",
      "Episode: 471 steps: 9\n",
      "Episode: 472 steps: 9\n",
      "Episode: 473 steps: 10\n",
      "Episode: 474 steps: 14\n",
      "Episode: 475 steps: 9\n",
      "Episode: 476 steps: 10\n",
      "Episode: 477 steps: 10\n",
      "Episode: 478 steps: 10\n",
      "Episode: 479 steps: 9\n",
      "Episode: 480 steps: 10\n",
      "Episode: 481 steps: 9\n",
      "Episode: 482 steps: 9\n",
      "Episode: 483 steps: 10\n",
      "Episode: 484 steps: 10\n",
      "Episode: 485 steps: 9\n",
      "Episode: 486 steps: 10\n",
      "Episode: 487 steps: 10\n",
      "Episode: 488 steps: 15\n",
      "Episode: 489 steps: 9\n",
      "Episode: 490 steps: 10\n",
      "Episode: 491 steps: 9\n",
      "Episode: 492 steps: 10\n",
      "Episode: 493 steps: 9\n",
      "Episode: 494 steps: 9\n",
      "Episode: 495 steps: 10\n",
      "Episode: 496 steps: 10\n",
      "Episode: 497 steps: 9\n",
      "Episode: 498 steps: 10\n",
      "Episode: 499 steps: 8\n",
      "Episode: 500 steps: 9\n",
      "Episode: 501 steps: 10\n",
      "Episode: 502 steps: 10\n",
      "Episode: 503 steps: 22\n",
      "Episode: 504 steps: 10\n",
      "Episode: 505 steps: 10\n",
      "Episode: 506 steps: 10\n",
      "Episode: 507 steps: 8\n",
      "Episode: 508 steps: 9\n",
      "Episode: 509 steps: 10\n",
      "Episode: 510 steps: 9\n",
      "Episode: 511 steps: 10\n",
      "Episode: 512 steps: 9\n",
      "Episode: 513 steps: 10\n",
      "Episode: 514 steps: 10\n",
      "Episode: 515 steps: 10\n",
      "Episode: 516 steps: 10\n",
      "Episode: 517 steps: 8\n",
      "Episode: 518 steps: 9\n",
      "Episode: 519 steps: 10\n",
      "Episode: 520 steps: 9\n",
      "Episode: 521 steps: 10\n",
      "Episode: 522 steps: 9\n",
      "Episode: 523 steps: 9\n",
      "Episode: 524 steps: 9\n",
      "Episode: 525 steps: 10\n",
      "Episode: 526 steps: 10\n",
      "Episode: 527 steps: 11\n",
      "Episode: 528 steps: 9\n",
      "Episode: 529 steps: 10\n",
      "Episode: 530 steps: 9\n",
      "Episode: 531 steps: 8\n",
      "Episode: 532 steps: 10\n",
      "Episode: 533 steps: 10\n",
      "Episode: 534 steps: 9\n",
      "Episode: 535 steps: 9\n",
      "Episode: 536 steps: 9\n",
      "Episode: 537 steps: 9\n",
      "Episode: 538 steps: 9\n",
      "Episode: 539 steps: 9\n",
      "Episode: 540 steps: 10\n",
      "Episode: 541 steps: 10\n",
      "Episode: 542 steps: 10\n",
      "Episode: 543 steps: 10\n",
      "Episode: 544 steps: 21\n",
      "Episode: 545 steps: 9\n",
      "Episode: 546 steps: 10\n",
      "Episode: 547 steps: 9\n",
      "Episode: 548 steps: 9\n",
      "Episode: 549 steps: 10\n",
      "Episode: 550 steps: 10\n",
      "Episode: 551 steps: 10\n",
      "Episode: 552 steps: 9\n",
      "Episode: 553 steps: 10\n",
      "Episode: 554 steps: 9\n",
      "Episode: 555 steps: 8\n",
      "Episode: 556 steps: 9\n",
      "Episode: 557 steps: 9\n",
      "Episode: 558 steps: 9\n",
      "Episode: 559 steps: 9\n",
      "Episode: 560 steps: 8\n",
      "Episode: 561 steps: 33\n",
      "Episode: 562 steps: 9\n",
      "Episode: 563 steps: 10\n",
      "Episode: 564 steps: 9\n",
      "Episode: 565 steps: 10\n",
      "Episode: 566 steps: 10\n",
      "Episode: 567 steps: 23\n",
      "Episode: 568 steps: 12\n",
      "Episode: 569 steps: 10\n",
      "Episode: 570 steps: 10\n",
      "Episode: 571 steps: 9\n",
      "Episode: 572 steps: 8\n",
      "Episode: 573 steps: 9\n",
      "Episode: 574 steps: 10\n",
      "Episode: 575 steps: 10\n",
      "Episode: 576 steps: 10\n",
      "Episode: 577 steps: 10\n",
      "Episode: 578 steps: 10\n",
      "Episode: 579 steps: 10\n",
      "Episode: 580 steps: 10\n",
      "Episode: 581 steps: 10\n",
      "Episode: 582 steps: 10\n",
      "Episode: 583 steps: 10\n",
      "Episode: 584 steps: 8\n",
      "Episode: 585 steps: 10\n",
      "Episode: 586 steps: 10\n",
      "Episode: 587 steps: 10\n",
      "Episode: 588 steps: 10\n",
      "Episode: 589 steps: 10\n",
      "Episode: 590 steps: 9\n",
      "Episode: 591 steps: 9\n",
      "Episode: 592 steps: 9\n",
      "Episode: 593 steps: 10\n",
      "Episode: 594 steps: 8\n",
      "Episode: 595 steps: 9\n",
      "Episode: 596 steps: 10\n",
      "Episode: 597 steps: 10\n",
      "Episode: 598 steps: 10\n",
      "Episode: 599 steps: 9\n",
      "Episode: 600 steps: 9\n",
      "Episode: 601 steps: 20\n",
      "Episode: 602 steps: 9\n",
      "Episode: 603 steps: 9\n",
      "Episode: 604 steps: 9\n",
      "Episode: 605 steps: 8\n",
      "Episode: 606 steps: 9\n",
      "Episode: 607 steps: 10\n",
      "Episode: 608 steps: 10\n",
      "Episode: 609 steps: 10\n",
      "Episode: 610 steps: 10\n",
      "Episode: 611 steps: 10\n",
      "Episode: 612 steps: 10\n",
      "Episode: 613 steps: 9\n",
      "Episode: 614 steps: 10\n",
      "Episode: 615 steps: 9\n",
      "Episode: 616 steps: 10\n",
      "Episode: 617 steps: 10\n",
      "Episode: 618 steps: 9\n",
      "Episode: 619 steps: 10\n",
      "Episode: 620 steps: 9\n",
      "Episode: 621 steps: 10\n",
      "Episode: 622 steps: 9\n",
      "Episode: 623 steps: 10\n",
      "Episode: 624 steps: 9\n",
      "Episode: 625 steps: 10\n",
      "Episode: 626 steps: 9\n",
      "Episode: 627 steps: 10\n",
      "Episode: 628 steps: 10\n",
      "Episode: 629 steps: 9\n",
      "Episode: 630 steps: 9\n",
      "Episode: 631 steps: 8\n",
      "Episode: 632 steps: 9\n",
      "Episode: 633 steps: 9\n",
      "Episode: 634 steps: 9\n",
      "Episode: 635 steps: 10\n",
      "Episode: 636 steps: 9\n",
      "Episode: 637 steps: 9\n",
      "Episode: 638 steps: 10\n",
      "Episode: 639 steps: 9\n",
      "Episode: 640 steps: 10\n",
      "Episode: 641 steps: 9\n",
      "Episode: 642 steps: 9\n",
      "Episode: 643 steps: 9\n",
      "Episode: 644 steps: 9\n",
      "Episode: 645 steps: 8\n",
      "Episode: 646 steps: 9\n",
      "Episode: 647 steps: 9\n",
      "Episode: 648 steps: 11\n",
      "Episode: 649 steps: 9\n",
      "Episode: 650 steps: 10\n",
      "Episode: 651 steps: 10\n",
      "Episode: 652 steps: 10\n",
      "Episode: 653 steps: 10\n",
      "Episode: 654 steps: 10\n",
      "Episode: 655 steps: 10\n",
      "Episode: 656 steps: 9\n",
      "Episode: 657 steps: 8\n",
      "Episode: 658 steps: 8\n",
      "Episode: 659 steps: 10\n",
      "Episode: 660 steps: 9\n",
      "Episode: 661 steps: 8\n",
      "Episode: 662 steps: 9\n",
      "Episode: 663 steps: 10\n",
      "Episode: 664 steps: 10\n",
      "Episode: 665 steps: 9\n",
      "Episode: 666 steps: 9\n",
      "Episode: 667 steps: 10\n",
      "Episode: 668 steps: 11\n",
      "Episode: 669 steps: 10\n",
      "Episode: 670 steps: 9\n",
      "Episode: 671 steps: 9\n",
      "Episode: 672 steps: 9\n",
      "Episode: 673 steps: 8\n",
      "Episode: 674 steps: 10\n",
      "Episode: 675 steps: 8\n",
      "Episode: 676 steps: 10\n",
      "Episode: 677 steps: 10\n",
      "Episode: 678 steps: 10\n",
      "Episode: 679 steps: 11\n",
      "Episode: 680 steps: 9\n",
      "Episode: 681 steps: 8\n",
      "Episode: 682 steps: 9\n",
      "Episode: 683 steps: 10\n",
      "Episode: 684 steps: 9\n",
      "Episode: 685 steps: 10\n",
      "Episode: 686 steps: 9\n",
      "Episode: 687 steps: 8\n",
      "Episode: 688 steps: 9\n",
      "Episode: 689 steps: 9\n",
      "Episode: 690 steps: 9\n",
      "Episode: 691 steps: 9\n",
      "Episode: 692 steps: 10\n",
      "Episode: 693 steps: 9\n",
      "Episode: 694 steps: 8\n",
      "Episode: 695 steps: 9\n",
      "Episode: 696 steps: 10\n",
      "Episode: 697 steps: 8\n",
      "Episode: 698 steps: 9\n",
      "Episode: 699 steps: 10\n",
      "Episode: 700 steps: 9\n",
      "Episode: 701 steps: 9\n",
      "Episode: 702 steps: 10\n",
      "Episode: 703 steps: 10\n",
      "Episode: 704 steps: 9\n",
      "Episode: 705 steps: 8\n",
      "Episode: 706 steps: 9\n",
      "Episode: 707 steps: 11\n",
      "Episode: 708 steps: 10\n",
      "Episode: 709 steps: 10\n",
      "Episode: 710 steps: 8\n",
      "Episode: 711 steps: 10\n",
      "Episode: 712 steps: 9\n",
      "Episode: 713 steps: 9\n",
      "Episode: 714 steps: 8\n",
      "Episode: 715 steps: 10\n",
      "Episode: 716 steps: 9\n",
      "Episode: 717 steps: 9\n",
      "Episode: 718 steps: 10\n",
      "Episode: 719 steps: 10\n",
      "Episode: 720 steps: 8\n",
      "Episode: 721 steps: 9\n",
      "Episode: 722 steps: 10\n",
      "Episode: 723 steps: 10\n",
      "Episode: 724 steps: 9\n",
      "Episode: 725 steps: 8\n",
      "Episode: 726 steps: 27\n",
      "Episode: 727 steps: 9\n",
      "Episode: 728 steps: 9\n",
      "Episode: 729 steps: 9\n",
      "Episode: 730 steps: 9\n",
      "Episode: 731 steps: 9\n",
      "Episode: 732 steps: 8\n",
      "Episode: 733 steps: 10\n",
      "Episode: 734 steps: 10\n",
      "Episode: 735 steps: 10\n",
      "Episode: 736 steps: 10\n",
      "Episode: 737 steps: 8\n",
      "Episode: 738 steps: 9\n",
      "Episode: 739 steps: 10\n",
      "Episode: 740 steps: 10\n",
      "Episode: 741 steps: 9\n",
      "Episode: 742 steps: 8\n",
      "Episode: 743 steps: 9\n",
      "Episode: 744 steps: 10\n",
      "Episode: 745 steps: 10\n",
      "Episode: 746 steps: 8\n",
      "Episode: 747 steps: 24\n",
      "Episode: 748 steps: 8\n",
      "Episode: 749 steps: 10\n",
      "Episode: 750 steps: 9\n",
      "Episode: 751 steps: 9\n",
      "Episode: 752 steps: 9\n",
      "Episode: 753 steps: 18\n",
      "Episode: 754 steps: 9\n",
      "Episode: 755 steps: 8\n",
      "Episode: 756 steps: 8\n",
      "Episode: 757 steps: 9\n",
      "Episode: 758 steps: 9\n",
      "Episode: 759 steps: 8\n",
      "Episode: 760 steps: 10\n",
      "Episode: 761 steps: 9\n",
      "Episode: 762 steps: 10\n",
      "Episode: 763 steps: 9\n",
      "Episode: 764 steps: 10\n",
      "Episode: 765 steps: 10\n",
      "Episode: 766 steps: 10\n",
      "Episode: 767 steps: 10\n",
      "Episode: 768 steps: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 769 steps: 9\n",
      "Episode: 770 steps: 8\n",
      "Episode: 771 steps: 9\n",
      "Episode: 772 steps: 8\n",
      "Episode: 773 steps: 10\n",
      "Episode: 774 steps: 9\n",
      "Episode: 775 steps: 10\n",
      "Episode: 776 steps: 9\n",
      "Episode: 777 steps: 10\n",
      "Episode: 778 steps: 10\n",
      "Episode: 779 steps: 10\n",
      "Episode: 780 steps: 10\n",
      "Episode: 781 steps: 10\n",
      "Episode: 782 steps: 12\n",
      "Episode: 783 steps: 9\n",
      "Episode: 784 steps: 10\n",
      "Episode: 785 steps: 8\n",
      "Episode: 786 steps: 9\n",
      "Episode: 787 steps: 10\n",
      "Episode: 788 steps: 9\n",
      "Episode: 789 steps: 10\n",
      "Episode: 790 steps: 10\n",
      "Episode: 791 steps: 9\n",
      "Episode: 792 steps: 10\n",
      "Episode: 793 steps: 10\n",
      "Episode: 794 steps: 10\n",
      "Episode: 795 steps: 8\n",
      "Episode: 796 steps: 9\n",
      "Episode: 797 steps: 10\n",
      "Episode: 798 steps: 9\n",
      "Episode: 799 steps: 10\n",
      "Episode: 800 steps: 10\n",
      "Episode: 801 steps: 10\n",
      "Episode: 802 steps: 9\n",
      "Episode: 803 steps: 9\n",
      "Episode: 804 steps: 17\n",
      "Episode: 805 steps: 9\n",
      "Episode: 806 steps: 8\n",
      "Episode: 807 steps: 9\n",
      "Episode: 808 steps: 9\n",
      "Episode: 809 steps: 10\n",
      "Episode: 810 steps: 9\n",
      "Episode: 811 steps: 10\n",
      "Episode: 812 steps: 10\n",
      "Episode: 813 steps: 10\n",
      "Episode: 814 steps: 9\n",
      "Episode: 815 steps: 8\n",
      "Episode: 816 steps: 9\n",
      "Episode: 817 steps: 8\n",
      "Episode: 818 steps: 8\n",
      "Episode: 819 steps: 10\n",
      "Episode: 820 steps: 9\n",
      "Episode: 821 steps: 8\n",
      "Episode: 822 steps: 9\n",
      "Episode: 823 steps: 10\n",
      "Episode: 824 steps: 9\n",
      "Episode: 825 steps: 9\n",
      "Episode: 826 steps: 10\n",
      "Episode: 827 steps: 9\n",
      "Episode: 828 steps: 9\n",
      "Episode: 829 steps: 10\n",
      "Episode: 830 steps: 9\n",
      "Episode: 831 steps: 10\n",
      "Episode: 832 steps: 10\n",
      "Episode: 833 steps: 10\n",
      "Episode: 834 steps: 9\n",
      "Episode: 835 steps: 9\n",
      "Episode: 836 steps: 9\n",
      "Episode: 837 steps: 9\n",
      "Episode: 838 steps: 9\n",
      "Episode: 839 steps: 10\n",
      "Episode: 840 steps: 8\n",
      "Episode: 841 steps: 19\n",
      "Episode: 842 steps: 10\n",
      "Episode: 843 steps: 9\n",
      "Episode: 844 steps: 10\n",
      "Episode: 845 steps: 10\n",
      "Episode: 846 steps: 9\n",
      "Episode: 847 steps: 10\n",
      "Episode: 848 steps: 9\n",
      "Episode: 849 steps: 9\n",
      "Episode: 850 steps: 9\n",
      "Episode: 851 steps: 9\n",
      "Episode: 852 steps: 10\n",
      "Episode: 853 steps: 9\n",
      "Episode: 854 steps: 9\n",
      "Episode: 855 steps: 8\n",
      "Episode: 856 steps: 9\n",
      "Episode: 857 steps: 9\n",
      "Episode: 858 steps: 9\n",
      "Episode: 859 steps: 10\n",
      "Episode: 860 steps: 10\n",
      "Episode: 861 steps: 10\n",
      "Episode: 862 steps: 9\n",
      "Episode: 863 steps: 10\n",
      "Episode: 864 steps: 9\n",
      "Episode: 865 steps: 10\n",
      "Episode: 866 steps: 10\n",
      "Episode: 867 steps: 8\n",
      "Episode: 868 steps: 8\n",
      "Episode: 869 steps: 11\n",
      "Episode: 870 steps: 10\n",
      "Episode: 871 steps: 10\n",
      "Episode: 872 steps: 10\n",
      "Episode: 873 steps: 10\n",
      "Episode: 874 steps: 9\n",
      "Episode: 875 steps: 10\n",
      "Episode: 876 steps: 9\n",
      "Episode: 877 steps: 10\n",
      "Episode: 878 steps: 10\n",
      "Episode: 879 steps: 11\n",
      "Episode: 880 steps: 9\n",
      "Episode: 881 steps: 10\n",
      "Episode: 882 steps: 9\n",
      "Episode: 883 steps: 8\n",
      "Episode: 884 steps: 10\n",
      "Episode: 885 steps: 10\n",
      "Episode: 886 steps: 10\n",
      "Episode: 887 steps: 8\n",
      "Episode: 888 steps: 9\n",
      "Episode: 889 steps: 10\n",
      "Episode: 890 steps: 18\n",
      "Episode: 891 steps: 8\n",
      "Episode: 892 steps: 9\n",
      "Episode: 893 steps: 10\n",
      "Episode: 894 steps: 9\n",
      "Episode: 895 steps: 10\n",
      "Episode: 896 steps: 10\n",
      "Episode: 897 steps: 16\n",
      "Episode: 898 steps: 8\n",
      "Episode: 899 steps: 10\n",
      "Episode: 900 steps: 14\n",
      "Episode: 901 steps: 8\n",
      "Episode: 902 steps: 9\n",
      "Episode: 903 steps: 10\n",
      "Episode: 904 steps: 8\n",
      "Episode: 905 steps: 9\n",
      "Episode: 906 steps: 11\n",
      "Episode: 907 steps: 10\n",
      "Episode: 908 steps: 10\n",
      "Episode: 909 steps: 10\n",
      "Episode: 910 steps: 9\n",
      "Episode: 911 steps: 9\n",
      "Episode: 912 steps: 10\n",
      "Episode: 913 steps: 10\n",
      "Episode: 914 steps: 10\n",
      "Episode: 915 steps: 8\n",
      "Episode: 916 steps: 10\n",
      "Episode: 917 steps: 9\n",
      "Episode: 918 steps: 10\n",
      "Episode: 919 steps: 10\n",
      "Episode: 920 steps: 10\n",
      "Episode: 921 steps: 13\n",
      "Episode: 922 steps: 9\n",
      "Episode: 923 steps: 10\n",
      "Episode: 924 steps: 9\n",
      "Episode: 925 steps: 10\n",
      "Episode: 926 steps: 10\n",
      "Episode: 927 steps: 17\n",
      "Episode: 928 steps: 9\n",
      "Episode: 929 steps: 9\n",
      "Episode: 930 steps: 8\n",
      "Episode: 931 steps: 10\n",
      "Episode: 932 steps: 8\n",
      "Episode: 933 steps: 8\n",
      "Episode: 934 steps: 11\n",
      "Episode: 935 steps: 9\n",
      "Episode: 936 steps: 9\n",
      "Episode: 937 steps: 9\n",
      "Episode: 938 steps: 9\n",
      "Episode: 939 steps: 9\n",
      "Episode: 940 steps: 10\n",
      "Episode: 941 steps: 9\n",
      "Episode: 942 steps: 10\n",
      "Episode: 943 steps: 10\n",
      "Episode: 944 steps: 8\n",
      "Episode: 945 steps: 10\n",
      "Episode: 946 steps: 9\n",
      "Episode: 947 steps: 9\n",
      "Episode: 948 steps: 8\n",
      "Episode: 949 steps: 9\n",
      "Episode: 950 steps: 9\n",
      "Episode: 951 steps: 10\n",
      "Episode: 952 steps: 8\n",
      "Episode: 953 steps: 8\n",
      "Episode: 954 steps: 10\n",
      "Episode: 955 steps: 10\n",
      "Episode: 956 steps: 8\n",
      "Episode: 957 steps: 9\n",
      "Episode: 958 steps: 10\n",
      "Episode: 959 steps: 10\n",
      "Episode: 960 steps: 8\n",
      "Episode: 961 steps: 9\n",
      "Episode: 962 steps: 10\n",
      "Episode: 963 steps: 8\n",
      "Episode: 964 steps: 10\n",
      "Episode: 965 steps: 11\n",
      "Episode: 966 steps: 9\n",
      "Episode: 967 steps: 11\n",
      "Episode: 968 steps: 9\n",
      "Episode: 969 steps: 9\n",
      "Episode: 970 steps: 10\n",
      "Episode: 971 steps: 10\n",
      "Episode: 972 steps: 9\n",
      "Episode: 973 steps: 10\n",
      "Episode: 974 steps: 10\n",
      "Episode: 975 steps: 9\n",
      "Episode: 976 steps: 10\n",
      "Episode: 977 steps: 9\n",
      "Episode: 978 steps: 9\n",
      "Episode: 979 steps: 11\n",
      "Episode: 980 steps: 25\n",
      "Episode: 981 steps: 10\n",
      "Episode: 982 steps: 8\n",
      "Episode: 983 steps: 8\n",
      "Episode: 984 steps: 9\n",
      "Episode: 985 steps: 9\n",
      "Episode: 986 steps: 8\n",
      "Episode: 987 steps: 8\n",
      "Episode: 988 steps: 10\n",
      "Episode: 989 steps: 10\n",
      "Episode: 990 steps: 10\n",
      "Episode: 991 steps: 14\n",
      "Episode: 992 steps: 9\n",
      "Episode: 993 steps: 9\n",
      "Episode: 994 steps: 9\n",
      "Episode: 995 steps: 9\n",
      "Episode: 996 steps: 10\n",
      "Episode: 997 steps: 9\n",
      "Episode: 998 steps: 10\n",
      "Episode: 999 steps: 9\n",
      "Episode: 1000 steps: 10\n",
      "Episode: 1001 steps: 10\n",
      "Episode: 1002 steps: 10\n",
      "Episode: 1003 steps: 8\n",
      "Episode: 1004 steps: 9\n",
      "Episode: 1005 steps: 10\n",
      "Episode: 1006 steps: 10\n",
      "Episode: 1007 steps: 9\n",
      "Episode: 1008 steps: 10\n",
      "Episode: 1009 steps: 10\n",
      "Episode: 1010 steps: 9\n",
      "Episode: 1011 steps: 9\n",
      "Episode: 1012 steps: 9\n",
      "Episode: 1013 steps: 8\n",
      "Episode: 1014 steps: 9\n",
      "Episode: 1015 steps: 8\n",
      "Episode: 1016 steps: 8\n",
      "Episode: 1017 steps: 13\n",
      "Episode: 1018 steps: 10\n",
      "Episode: 1019 steps: 10\n",
      "Episode: 1020 steps: 9\n",
      "Episode: 1021 steps: 9\n",
      "Episode: 1022 steps: 8\n",
      "Episode: 1023 steps: 10\n",
      "Episode: 1024 steps: 10\n",
      "Episode: 1025 steps: 10\n",
      "Episode: 1026 steps: 9\n",
      "Episode: 1027 steps: 8\n",
      "Episode: 1028 steps: 25\n",
      "Episode: 1029 steps: 9\n",
      "Episode: 1030 steps: 26\n",
      "Episode: 1031 steps: 8\n",
      "Episode: 1032 steps: 10\n",
      "Episode: 1033 steps: 9\n",
      "Episode: 1034 steps: 9\n",
      "Episode: 1035 steps: 10\n",
      "Episode: 1036 steps: 10\n",
      "Episode: 1037 steps: 9\n",
      "Episode: 1038 steps: 10\n",
      "Episode: 1039 steps: 10\n",
      "Episode: 1040 steps: 10\n",
      "Episode: 1041 steps: 10\n",
      "Episode: 1042 steps: 14\n",
      "Episode: 1043 steps: 9\n",
      "Episode: 1044 steps: 10\n",
      "Episode: 1045 steps: 17\n",
      "Episode: 1046 steps: 9\n",
      "Episode: 1047 steps: 10\n",
      "Episode: 1048 steps: 10\n",
      "Episode: 1049 steps: 10\n",
      "Episode: 1050 steps: 8\n",
      "Episode: 1051 steps: 9\n",
      "Episode: 1052 steps: 8\n",
      "Episode: 1053 steps: 8\n",
      "Episode: 1054 steps: 9\n",
      "Episode: 1055 steps: 10\n",
      "Episode: 1056 steps: 10\n",
      "Episode: 1057 steps: 9\n",
      "Episode: 1058 steps: 10\n",
      "Episode: 1059 steps: 10\n",
      "Episode: 1060 steps: 8\n",
      "Episode: 1061 steps: 9\n",
      "Episode: 1062 steps: 10\n",
      "Episode: 1063 steps: 10\n",
      "Episode: 1064 steps: 24\n",
      "Episode: 1065 steps: 9\n",
      "Episode: 1066 steps: 8\n",
      "Episode: 1067 steps: 10\n",
      "Episode: 1068 steps: 10\n",
      "Episode: 1069 steps: 10\n",
      "Episode: 1070 steps: 10\n",
      "Episode: 1071 steps: 8\n",
      "Episode: 1072 steps: 9\n",
      "Episode: 1073 steps: 10\n",
      "Episode: 1074 steps: 10\n",
      "Episode: 1075 steps: 9\n",
      "Episode: 1076 steps: 9\n",
      "Episode: 1077 steps: 9\n",
      "Episode: 1078 steps: 9\n",
      "Episode: 1079 steps: 11\n",
      "Episode: 1080 steps: 10\n",
      "Episode: 1081 steps: 10\n",
      "Episode: 1082 steps: 10\n",
      "Episode: 1083 steps: 10\n",
      "Episode: 1084 steps: 8\n",
      "Episode: 1085 steps: 9\n",
      "Episode: 1086 steps: 13\n",
      "Episode: 1087 steps: 10\n",
      "Episode: 1088 steps: 9\n",
      "Episode: 1089 steps: 9\n",
      "Episode: 1090 steps: 10\n",
      "Episode: 1091 steps: 10\n",
      "Episode: 1092 steps: 10\n",
      "Episode: 1093 steps: 20\n",
      "Episode: 1094 steps: 9\n",
      "Episode: 1095 steps: 9\n",
      "Episode: 1096 steps: 8\n",
      "Episode: 1097 steps: 9\n",
      "Episode: 1098 steps: 9\n",
      "Episode: 1099 steps: 10\n",
      "Episode: 1100 steps: 8\n",
      "Episode: 1101 steps: 18\n",
      "Episode: 1102 steps: 9\n",
      "Episode: 1103 steps: 9\n",
      "Episode: 1104 steps: 9\n",
      "Episode: 1105 steps: 10\n",
      "Episode: 1106 steps: 9\n",
      "Episode: 1107 steps: 9\n",
      "Episode: 1108 steps: 9\n",
      "Episode: 1109 steps: 10\n",
      "Episode: 1110 steps: 10\n",
      "Episode: 1111 steps: 10\n",
      "Episode: 1112 steps: 8\n",
      "Episode: 1113 steps: 10\n",
      "Episode: 1114 steps: 9\n",
      "Episode: 1115 steps: 9\n",
      "Episode: 1116 steps: 9\n",
      "Episode: 1117 steps: 9\n",
      "Episode: 1118 steps: 10\n",
      "Episode: 1119 steps: 9\n",
      "Episode: 1120 steps: 10\n",
      "Episode: 1121 steps: 10\n",
      "Episode: 1122 steps: 9\n",
      "Episode: 1123 steps: 9\n",
      "Episode: 1124 steps: 8\n",
      "Episode: 1125 steps: 9\n",
      "Episode: 1126 steps: 10\n",
      "Episode: 1127 steps: 10\n",
      "Episode: 1128 steps: 8\n",
      "Episode: 1129 steps: 10\n",
      "Episode: 1130 steps: 10\n",
      "Episode: 1131 steps: 9\n",
      "Episode: 1132 steps: 24\n",
      "Episode: 1133 steps: 9\n",
      "Episode: 1134 steps: 10\n",
      "Episode: 1135 steps: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1136 steps: 9\n",
      "Episode: 1137 steps: 8\n",
      "Episode: 1138 steps: 9\n",
      "Episode: 1139 steps: 10\n",
      "Episode: 1140 steps: 11\n",
      "Episode: 1141 steps: 10\n",
      "Episode: 1142 steps: 8\n",
      "Episode: 1143 steps: 9\n",
      "Episode: 1144 steps: 10\n",
      "Episode: 1145 steps: 9\n",
      "Episode: 1146 steps: 9\n",
      "Episode: 1147 steps: 9\n",
      "Episode: 1148 steps: 11\n",
      "Episode: 1149 steps: 8\n",
      "Episode: 1150 steps: 9\n",
      "Episode: 1151 steps: 8\n",
      "Episode: 1152 steps: 10\n",
      "Episode: 1153 steps: 9\n",
      "Episode: 1154 steps: 10\n",
      "Episode: 1155 steps: 10\n",
      "Episode: 1156 steps: 10\n",
      "Episode: 1157 steps: 9\n",
      "Episode: 1158 steps: 10\n",
      "Episode: 1159 steps: 10\n",
      "Episode: 1160 steps: 10\n",
      "Episode: 1161 steps: 10\n",
      "Episode: 1162 steps: 9\n",
      "Episode: 1163 steps: 9\n",
      "Episode: 1164 steps: 8\n",
      "Episode: 1165 steps: 8\n",
      "Episode: 1166 steps: 10\n",
      "Episode: 1167 steps: 10\n",
      "Episode: 1168 steps: 9\n",
      "Episode: 1169 steps: 10\n",
      "Episode: 1170 steps: 9\n",
      "Episode: 1171 steps: 9\n",
      "Episode: 1172 steps: 10\n",
      "Episode: 1173 steps: 10\n",
      "Episode: 1174 steps: 9\n",
      "Episode: 1175 steps: 10\n",
      "Episode: 1176 steps: 10\n",
      "Episode: 1177 steps: 10\n",
      "Episode: 1178 steps: 9\n",
      "Episode: 1179 steps: 9\n",
      "Episode: 1180 steps: 9\n",
      "Episode: 1181 steps: 10\n",
      "Episode: 1182 steps: 10\n",
      "Episode: 1183 steps: 10\n",
      "Episode: 1184 steps: 9\n",
      "Episode: 1185 steps: 11\n",
      "Episode: 1186 steps: 10\n",
      "Episode: 1187 steps: 9\n",
      "Episode: 1188 steps: 10\n",
      "Episode: 1189 steps: 9\n",
      "Episode: 1190 steps: 10\n",
      "Episode: 1191 steps: 9\n",
      "Episode: 1192 steps: 9\n",
      "Episode: 1193 steps: 9\n",
      "Episode: 1194 steps: 8\n",
      "Episode: 1195 steps: 9\n",
      "Episode: 1196 steps: 10\n",
      "Episode: 1197 steps: 10\n",
      "Episode: 1198 steps: 10\n",
      "Episode: 1199 steps: 9\n",
      "Episode: 1200 steps: 11\n",
      "Episode: 1201 steps: 9\n",
      "Episode: 1202 steps: 9\n",
      "Episode: 1203 steps: 8\n",
      "Episode: 1204 steps: 10\n",
      "Episode: 1205 steps: 9\n",
      "Episode: 1206 steps: 8\n",
      "Episode: 1207 steps: 10\n",
      "Episode: 1208 steps: 9\n",
      "Episode: 1209 steps: 8\n",
      "Episode: 1210 steps: 9\n",
      "Episode: 1211 steps: 10\n",
      "Episode: 1212 steps: 8\n",
      "Episode: 1213 steps: 10\n",
      "Episode: 1214 steps: 8\n",
      "Episode: 1215 steps: 9\n",
      "Episode: 1216 steps: 9\n",
      "Episode: 1217 steps: 9\n",
      "Episode: 1218 steps: 10\n",
      "Episode: 1219 steps: 10\n",
      "Episode: 1220 steps: 8\n",
      "Episode: 1221 steps: 9\n",
      "Episode: 1222 steps: 9\n",
      "Episode: 1223 steps: 9\n",
      "Episode: 1224 steps: 10\n",
      "Episode: 1225 steps: 10\n",
      "Episode: 1226 steps: 9\n",
      "Episode: 1227 steps: 13\n",
      "Episode: 1228 steps: 10\n",
      "Episode: 1229 steps: 8\n",
      "Episode: 1230 steps: 8\n",
      "Episode: 1231 steps: 10\n",
      "Episode: 1232 steps: 9\n",
      "Episode: 1233 steps: 10\n",
      "Episode: 1234 steps: 10\n",
      "Episode: 1235 steps: 9\n",
      "Episode: 1236 steps: 10\n",
      "Episode: 1237 steps: 9\n",
      "Episode: 1238 steps: 10\n",
      "Episode: 1239 steps: 11\n",
      "Episode: 1240 steps: 10\n",
      "Episode: 1241 steps: 10\n",
      "Episode: 1242 steps: 10\n",
      "Episode: 1243 steps: 10\n",
      "Episode: 1244 steps: 10\n",
      "Episode: 1245 steps: 11\n",
      "Episode: 1246 steps: 10\n",
      "Episode: 1247 steps: 9\n",
      "Episode: 1248 steps: 10\n",
      "Episode: 1249 steps: 10\n",
      "Episode: 1250 steps: 10\n",
      "Episode: 1251 steps: 10\n",
      "Episode: 1252 steps: 10\n",
      "Episode: 1253 steps: 9\n",
      "Episode: 1254 steps: 10\n",
      "Episode: 1255 steps: 10\n",
      "Episode: 1256 steps: 9\n",
      "Episode: 1257 steps: 9\n",
      "Episode: 1258 steps: 9\n",
      "Episode: 1259 steps: 9\n",
      "Episode: 1260 steps: 9\n",
      "Episode: 1261 steps: 10\n",
      "Episode: 1262 steps: 9\n",
      "Episode: 1263 steps: 10\n",
      "Episode: 1264 steps: 10\n",
      "Episode: 1265 steps: 9\n",
      "Episode: 1266 steps: 9\n",
      "Episode: 1267 steps: 9\n",
      "Episode: 1268 steps: 8\n",
      "Episode: 1269 steps: 10\n",
      "Episode: 1270 steps: 11\n",
      "Episode: 1271 steps: 8\n",
      "Episode: 1272 steps: 10\n",
      "Episode: 1273 steps: 10\n",
      "Episode: 1274 steps: 9\n",
      "Episode: 1275 steps: 9\n",
      "Episode: 1276 steps: 10\n",
      "Episode: 1277 steps: 10\n",
      "Episode: 1278 steps: 8\n",
      "Episode: 1279 steps: 9\n",
      "Episode: 1280 steps: 10\n",
      "Episode: 1281 steps: 9\n",
      "Episode: 1282 steps: 9\n",
      "Episode: 1283 steps: 9\n",
      "Episode: 1284 steps: 9\n",
      "Episode: 1285 steps: 10\n",
      "Episode: 1286 steps: 8\n",
      "Episode: 1287 steps: 10\n",
      "Episode: 1288 steps: 9\n",
      "Episode: 1289 steps: 9\n",
      "Episode: 1290 steps: 9\n",
      "Episode: 1291 steps: 9\n",
      "Episode: 1292 steps: 8\n",
      "Episode: 1293 steps: 9\n",
      "Episode: 1294 steps: 11\n",
      "Episode: 1295 steps: 8\n",
      "Episode: 1296 steps: 9\n",
      "Episode: 1297 steps: 8\n",
      "Episode: 1298 steps: 8\n",
      "Episode: 1299 steps: 10\n",
      "Episode: 1300 steps: 8\n",
      "Episode: 1301 steps: 9\n",
      "Episode: 1302 steps: 11\n",
      "Episode: 1303 steps: 9\n",
      "Episode: 1304 steps: 10\n",
      "Episode: 1305 steps: 9\n",
      "Episode: 1306 steps: 10\n",
      "Episode: 1307 steps: 10\n",
      "Episode: 1308 steps: 10\n",
      "Episode: 1309 steps: 9\n",
      "Episode: 1310 steps: 9\n",
      "Episode: 1311 steps: 8\n",
      "Episode: 1312 steps: 8\n",
      "Episode: 1313 steps: 10\n",
      "Episode: 1314 steps: 9\n",
      "Episode: 1315 steps: 10\n",
      "Episode: 1316 steps: 9\n",
      "Episode: 1317 steps: 10\n",
      "Episode: 1318 steps: 9\n",
      "Episode: 1319 steps: 8\n",
      "Episode: 1320 steps: 9\n",
      "Episode: 1321 steps: 9\n",
      "Episode: 1322 steps: 10\n",
      "Episode: 1323 steps: 9\n",
      "Episode: 1324 steps: 11\n",
      "Episode: 1325 steps: 9\n",
      "Episode: 1326 steps: 8\n",
      "Episode: 1327 steps: 11\n",
      "Episode: 1328 steps: 10\n",
      "Episode: 1329 steps: 10\n",
      "Episode: 1330 steps: 10\n",
      "Episode: 1331 steps: 10\n",
      "Episode: 1332 steps: 10\n",
      "Episode: 1333 steps: 10\n",
      "Episode: 1334 steps: 10\n",
      "Episode: 1335 steps: 9\n",
      "Episode: 1336 steps: 19\n",
      "Episode: 1337 steps: 9\n",
      "Episode: 1338 steps: 10\n",
      "Episode: 1339 steps: 10\n",
      "Episode: 1340 steps: 10\n",
      "Episode: 1341 steps: 9\n",
      "Episode: 1342 steps: 10\n",
      "Episode: 1343 steps: 10\n",
      "Episode: 1344 steps: 10\n",
      "Episode: 1345 steps: 9\n",
      "Episode: 1346 steps: 10\n",
      "Episode: 1347 steps: 10\n",
      "Episode: 1348 steps: 10\n",
      "Episode: 1349 steps: 9\n",
      "Episode: 1350 steps: 9\n",
      "Episode: 1351 steps: 8\n",
      "Episode: 1352 steps: 10\n",
      "Episode: 1353 steps: 10\n",
      "Episode: 1354 steps: 10\n",
      "Episode: 1355 steps: 9\n",
      "Episode: 1356 steps: 10\n",
      "Episode: 1357 steps: 10\n",
      "Episode: 1358 steps: 9\n",
      "Episode: 1359 steps: 8\n",
      "Episode: 1360 steps: 9\n",
      "Episode: 1361 steps: 10\n",
      "Episode: 1362 steps: 9\n",
      "Episode: 1363 steps: 8\n",
      "Episode: 1364 steps: 9\n",
      "Episode: 1365 steps: 9\n",
      "Episode: 1366 steps: 10\n",
      "Episode: 1367 steps: 10\n",
      "Episode: 1368 steps: 8\n",
      "Episode: 1369 steps: 9\n",
      "Episode: 1370 steps: 9\n",
      "Episode: 1371 steps: 8\n",
      "Episode: 1372 steps: 8\n",
      "Episode: 1373 steps: 10\n",
      "Episode: 1374 steps: 9\n",
      "Episode: 1375 steps: 9\n",
      "Episode: 1376 steps: 8\n",
      "Episode: 1377 steps: 8\n",
      "Episode: 1378 steps: 9\n",
      "Episode: 1379 steps: 10\n",
      "Episode: 1380 steps: 10\n",
      "Episode: 1381 steps: 10\n",
      "Episode: 1382 steps: 9\n",
      "Episode: 1383 steps: 9\n",
      "Episode: 1384 steps: 8\n",
      "Episode: 1385 steps: 9\n",
      "Episode: 1386 steps: 8\n",
      "Episode: 1387 steps: 11\n",
      "Episode: 1388 steps: 9\n",
      "Episode: 1389 steps: 9\n",
      "Episode: 1390 steps: 10\n",
      "Episode: 1391 steps: 10\n",
      "Episode: 1392 steps: 9\n",
      "Episode: 1393 steps: 10\n",
      "Episode: 1394 steps: 8\n",
      "Episode: 1395 steps: 10\n",
      "Episode: 1396 steps: 10\n",
      "Episode: 1397 steps: 10\n",
      "Episode: 1398 steps: 10\n",
      "Episode: 1399 steps: 10\n",
      "Episode: 1400 steps: 9\n",
      "Episode: 1401 steps: 9\n",
      "Episode: 1402 steps: 9\n",
      "Episode: 1403 steps: 10\n",
      "Episode: 1404 steps: 8\n",
      "Episode: 1405 steps: 10\n",
      "Episode: 1406 steps: 10\n",
      "Episode: 1407 steps: 9\n",
      "Episode: 1408 steps: 10\n",
      "Episode: 1409 steps: 9\n",
      "Episode: 1410 steps: 8\n",
      "Episode: 1411 steps: 9\n",
      "Episode: 1412 steps: 10\n",
      "Episode: 1413 steps: 10\n",
      "Episode: 1414 steps: 10\n",
      "Episode: 1415 steps: 10\n",
      "Episode: 1416 steps: 10\n",
      "Episode: 1417 steps: 10\n",
      "Episode: 1418 steps: 10\n",
      "Episode: 1419 steps: 10\n",
      "Episode: 1420 steps: 8\n",
      "Episode: 1421 steps: 8\n",
      "Episode: 1422 steps: 9\n",
      "Episode: 1423 steps: 10\n",
      "Episode: 1424 steps: 9\n",
      "Episode: 1425 steps: 10\n",
      "Episode: 1426 steps: 10\n",
      "Episode: 1427 steps: 9\n",
      "Episode: 1428 steps: 10\n",
      "Episode: 1429 steps: 8\n",
      "Episode: 1430 steps: 10\n",
      "Episode: 1431 steps: 8\n",
      "Episode: 1432 steps: 10\n",
      "Episode: 1433 steps: 8\n",
      "Episode: 1434 steps: 10\n",
      "Episode: 1435 steps: 10\n",
      "Episode: 1436 steps: 9\n",
      "Episode: 1437 steps: 10\n",
      "Episode: 1438 steps: 10\n",
      "Episode: 1439 steps: 10\n",
      "Episode: 1440 steps: 9\n",
      "Episode: 1441 steps: 10\n",
      "Episode: 1442 steps: 9\n",
      "Episode: 1443 steps: 10\n",
      "Episode: 1444 steps: 8\n",
      "Episode: 1445 steps: 11\n",
      "Episode: 1446 steps: 9\n",
      "Episode: 1447 steps: 10\n",
      "Episode: 1448 steps: 10\n",
      "Episode: 1449 steps: 9\n",
      "Episode: 1450 steps: 9\n",
      "Episode: 1451 steps: 8\n",
      "Episode: 1452 steps: 10\n",
      "Episode: 1453 steps: 10\n",
      "Episode: 1454 steps: 10\n",
      "Episode: 1455 steps: 8\n",
      "Episode: 1456 steps: 10\n",
      "Episode: 1457 steps: 9\n",
      "Episode: 1458 steps: 13\n",
      "Episode: 1459 steps: 8\n",
      "Episode: 1460 steps: 10\n",
      "Episode: 1461 steps: 10\n",
      "Episode: 1462 steps: 11\n",
      "Episode: 1463 steps: 10\n",
      "Episode: 1464 steps: 10\n",
      "Episode: 1465 steps: 9\n",
      "Episode: 1466 steps: 9\n",
      "Episode: 1467 steps: 9\n",
      "Episode: 1468 steps: 8\n",
      "Episode: 1469 steps: 9\n",
      "Episode: 1470 steps: 8\n",
      "Episode: 1471 steps: 8\n",
      "Episode: 1472 steps: 10\n",
      "Episode: 1473 steps: 8\n",
      "Episode: 1474 steps: 9\n",
      "Episode: 1475 steps: 10\n",
      "Episode: 1476 steps: 9\n",
      "Episode: 1477 steps: 10\n",
      "Episode: 1478 steps: 9\n",
      "Episode: 1479 steps: 10\n",
      "Episode: 1480 steps: 10\n",
      "Episode: 1481 steps: 10\n",
      "Episode: 1482 steps: 10\n",
      "Episode: 1483 steps: 9\n",
      "Episode: 1484 steps: 10\n",
      "Episode: 1485 steps: 9\n",
      "Episode: 1486 steps: 8\n",
      "Episode: 1487 steps: 9\n",
      "Episode: 1488 steps: 10\n",
      "Episode: 1489 steps: 9\n",
      "Episode: 1490 steps: 10\n",
      "Episode: 1491 steps: 9\n",
      "Episode: 1492 steps: 9\n",
      "Episode: 1493 steps: 9\n",
      "Episode: 1494 steps: 10\n",
      "Episode: 1495 steps: 10\n",
      "Episode: 1496 steps: 10\n",
      "Episode: 1497 steps: 9\n",
      "Episode: 1498 steps: 9\n",
      "Episode: 1499 steps: 9\n",
      "Episode: 1500 steps: 8\n",
      "Episode: 1501 steps: 11\n",
      "Episode: 1502 steps: 9\n",
      "Episode: 1503 steps: 10\n",
      "Episode: 1504 steps: 10\n",
      "Episode: 1505 steps: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1506 steps: 10\n",
      "Episode: 1507 steps: 10\n",
      "Episode: 1508 steps: 10\n",
      "Episode: 1509 steps: 9\n",
      "Episode: 1510 steps: 9\n",
      "Episode: 1511 steps: 9\n",
      "Episode: 1512 steps: 9\n",
      "Episode: 1513 steps: 9\n",
      "Episode: 1514 steps: 8\n",
      "Episode: 1515 steps: 10\n",
      "Episode: 1516 steps: 10\n",
      "Episode: 1517 steps: 9\n",
      "Episode: 1518 steps: 9\n",
      "Episode: 1519 steps: 10\n",
      "Episode: 1520 steps: 10\n",
      "Episode: 1521 steps: 9\n",
      "Episode: 1522 steps: 10\n",
      "Episode: 1523 steps: 10\n",
      "Episode: 1524 steps: 8\n",
      "Episode: 1525 steps: 9\n",
      "Episode: 1526 steps: 9\n",
      "Episode: 1527 steps: 8\n",
      "Episode: 1528 steps: 8\n",
      "Episode: 1529 steps: 10\n",
      "Episode: 1530 steps: 9\n",
      "Episode: 1531 steps: 22\n",
      "Episode: 1532 steps: 10\n",
      "Episode: 1533 steps: 9\n",
      "Episode: 1534 steps: 8\n",
      "Episode: 1535 steps: 9\n",
      "Episode: 1536 steps: 10\n",
      "Episode: 1537 steps: 10\n",
      "Episode: 1538 steps: 10\n",
      "Episode: 1539 steps: 11\n",
      "Episode: 1540 steps: 9\n",
      "Episode: 1541 steps: 9\n",
      "Episode: 1542 steps: 9\n",
      "Episode: 1543 steps: 10\n",
      "Episode: 1544 steps: 11\n",
      "Episode: 1545 steps: 10\n",
      "Episode: 1546 steps: 9\n",
      "Episode: 1547 steps: 10\n",
      "Episode: 1548 steps: 9\n",
      "Episode: 1549 steps: 10\n",
      "Episode: 1550 steps: 9\n",
      "Episode: 1551 steps: 10\n",
      "Episode: 1552 steps: 10\n",
      "Episode: 1553 steps: 9\n",
      "Episode: 1554 steps: 9\n",
      "Episode: 1555 steps: 9\n",
      "Episode: 1556 steps: 9\n",
      "Episode: 1557 steps: 10\n",
      "Episode: 1558 steps: 8\n",
      "Episode: 1559 steps: 8\n",
      "Episode: 1560 steps: 10\n",
      "Episode: 1561 steps: 8\n",
      "Episode: 1562 steps: 9\n",
      "Episode: 1563 steps: 9\n",
      "Episode: 1564 steps: 10\n",
      "Episode: 1565 steps: 10\n",
      "Episode: 1566 steps: 10\n",
      "Episode: 1567 steps: 10\n",
      "Episode: 1568 steps: 9\n",
      "Episode: 1569 steps: 9\n",
      "Episode: 1570 steps: 10\n",
      "Episode: 1571 steps: 9\n",
      "Episode: 1572 steps: 10\n",
      "Episode: 1573 steps: 9\n",
      "Episode: 1574 steps: 10\n",
      "Episode: 1575 steps: 10\n",
      "Episode: 1576 steps: 9\n",
      "Episode: 1577 steps: 10\n",
      "Episode: 1578 steps: 10\n",
      "Episode: 1579 steps: 9\n",
      "Episode: 1580 steps: 8\n",
      "Episode: 1581 steps: 10\n",
      "Episode: 1582 steps: 10\n",
      "Episode: 1583 steps: 10\n",
      "Episode: 1584 steps: 9\n",
      "Episode: 1585 steps: 10\n",
      "Episode: 1586 steps: 9\n",
      "Episode: 1587 steps: 10\n",
      "Episode: 1588 steps: 10\n",
      "Episode: 1589 steps: 10\n",
      "Episode: 1590 steps: 10\n",
      "Episode: 1591 steps: 9\n",
      "Episode: 1592 steps: 9\n",
      "Episode: 1593 steps: 10\n",
      "Episode: 1594 steps: 8\n",
      "Episode: 1595 steps: 10\n",
      "Episode: 1596 steps: 9\n",
      "Episode: 1597 steps: 10\n",
      "Episode: 1598 steps: 8\n",
      "Episode: 1599 steps: 11\n",
      "Episode: 1600 steps: 9\n",
      "Episode: 1601 steps: 9\n",
      "Episode: 1602 steps: 10\n",
      "Episode: 1603 steps: 10\n",
      "Episode: 1604 steps: 10\n",
      "Episode: 1605 steps: 9\n",
      "Episode: 1606 steps: 10\n",
      "Episode: 1607 steps: 10\n",
      "Episode: 1608 steps: 10\n",
      "Episode: 1609 steps: 9\n",
      "Episode: 1610 steps: 9\n",
      "Episode: 1611 steps: 8\n",
      "Episode: 1612 steps: 8\n",
      "Episode: 1613 steps: 9\n",
      "Episode: 1614 steps: 12\n",
      "Episode: 1615 steps: 8\n",
      "Episode: 1616 steps: 9\n",
      "Episode: 1617 steps: 9\n",
      "Episode: 1618 steps: 9\n",
      "Episode: 1619 steps: 10\n",
      "Episode: 1620 steps: 8\n",
      "Episode: 1621 steps: 10\n",
      "Episode: 1622 steps: 10\n",
      "Episode: 1623 steps: 9\n",
      "Episode: 1624 steps: 15\n",
      "Episode: 1625 steps: 9\n",
      "Episode: 1626 steps: 9\n",
      "Episode: 1627 steps: 10\n",
      "Episode: 1628 steps: 10\n",
      "Episode: 1629 steps: 10\n",
      "Episode: 1630 steps: 9\n",
      "Episode: 1631 steps: 15\n",
      "Episode: 1632 steps: 9\n",
      "Episode: 1633 steps: 10\n",
      "Episode: 1634 steps: 10\n",
      "Episode: 1635 steps: 8\n",
      "Episode: 1636 steps: 17\n",
      "Episode: 1637 steps: 10\n",
      "Episode: 1638 steps: 9\n",
      "Episode: 1639 steps: 9\n",
      "Episode: 1640 steps: 9\n",
      "Episode: 1641 steps: 11\n",
      "Episode: 1642 steps: 10\n",
      "Episode: 1643 steps: 9\n",
      "Episode: 1644 steps: 9\n",
      "Episode: 1645 steps: 9\n",
      "Episode: 1646 steps: 8\n",
      "Episode: 1647 steps: 10\n",
      "Episode: 1648 steps: 10\n",
      "Episode: 1649 steps: 9\n",
      "Episode: 1650 steps: 10\n",
      "Episode: 1651 steps: 9\n",
      "Episode: 1652 steps: 10\n",
      "Episode: 1653 steps: 8\n",
      "Episode: 1654 steps: 11\n",
      "Episode: 1655 steps: 9\n",
      "Episode: 1656 steps: 8\n",
      "Episode: 1657 steps: 9\n",
      "Episode: 1658 steps: 10\n",
      "Episode: 1659 steps: 10\n",
      "Episode: 1660 steps: 10\n",
      "Episode: 1661 steps: 9\n",
      "Episode: 1662 steps: 10\n",
      "Episode: 1663 steps: 8\n",
      "Episode: 1664 steps: 9\n",
      "Episode: 1665 steps: 9\n",
      "Episode: 1666 steps: 10\n",
      "Episode: 1667 steps: 10\n",
      "Episode: 1668 steps: 9\n",
      "Episode: 1669 steps: 22\n",
      "Episode: 1670 steps: 9\n",
      "Episode: 1671 steps: 9\n",
      "Episode: 1672 steps: 8\n",
      "Episode: 1673 steps: 9\n",
      "Episode: 1674 steps: 10\n",
      "Episode: 1675 steps: 10\n",
      "Episode: 1676 steps: 10\n",
      "Episode: 1677 steps: 8\n",
      "Episode: 1678 steps: 8\n",
      "Episode: 1679 steps: 9\n",
      "Episode: 1680 steps: 9\n",
      "Episode: 1681 steps: 8\n",
      "Episode: 1682 steps: 10\n",
      "Episode: 1683 steps: 9\n",
      "Episode: 1684 steps: 11\n",
      "Episode: 1685 steps: 9\n",
      "Episode: 1686 steps: 8\n",
      "Episode: 1687 steps: 10\n",
      "Episode: 1688 steps: 9\n",
      "Episode: 1689 steps: 8\n",
      "Episode: 1690 steps: 9\n",
      "Episode: 1691 steps: 9\n",
      "Episode: 1692 steps: 8\n",
      "Episode: 1693 steps: 9\n",
      "Episode: 1694 steps: 10\n",
      "Episode: 1695 steps: 10\n",
      "Episode: 1696 steps: 10\n",
      "Episode: 1697 steps: 10\n",
      "Episode: 1698 steps: 10\n",
      "Episode: 1699 steps: 10\n",
      "Episode: 1700 steps: 10\n",
      "Episode: 1701 steps: 10\n",
      "Episode: 1702 steps: 10\n",
      "Episode: 1703 steps: 10\n",
      "Episode: 1704 steps: 10\n",
      "Episode: 1705 steps: 9\n",
      "Episode: 1706 steps: 10\n",
      "Episode: 1707 steps: 9\n",
      "Episode: 1708 steps: 9\n",
      "Episode: 1709 steps: 11\n",
      "Episode: 1710 steps: 10\n",
      "Episode: 1711 steps: 10\n",
      "Episode: 1712 steps: 10\n",
      "Episode: 1713 steps: 12\n",
      "Episode: 1714 steps: 8\n",
      "Episode: 1715 steps: 9\n",
      "Episode: 1716 steps: 9\n",
      "Episode: 1717 steps: 9\n",
      "Episode: 1718 steps: 9\n",
      "Episode: 1719 steps: 10\n",
      "Episode: 1720 steps: 10\n",
      "Episode: 1721 steps: 8\n",
      "Episode: 1722 steps: 9\n",
      "Episode: 1723 steps: 10\n",
      "Episode: 1724 steps: 9\n",
      "Episode: 1725 steps: 9\n",
      "Episode: 1726 steps: 9\n",
      "Episode: 1727 steps: 9\n",
      "Episode: 1728 steps: 9\n",
      "Episode: 1729 steps: 9\n",
      "Episode: 1730 steps: 9\n",
      "Episode: 1731 steps: 10\n",
      "Episode: 1732 steps: 10\n",
      "Episode: 1733 steps: 10\n",
      "Episode: 1734 steps: 10\n",
      "Episode: 1735 steps: 10\n",
      "Episode: 1736 steps: 10\n",
      "Episode: 1737 steps: 8\n",
      "Episode: 1738 steps: 10\n",
      "Episode: 1739 steps: 10\n",
      "Episode: 1740 steps: 10\n",
      "Episode: 1741 steps: 10\n",
      "Episode: 1742 steps: 9\n",
      "Episode: 1743 steps: 8\n",
      "Episode: 1744 steps: 10\n",
      "Episode: 1745 steps: 10\n",
      "Episode: 1746 steps: 9\n",
      "Episode: 1747 steps: 10\n",
      "Episode: 1748 steps: 9\n",
      "Episode: 1749 steps: 11\n",
      "Episode: 1750 steps: 10\n",
      "Episode: 1751 steps: 8\n",
      "Episode: 1752 steps: 10\n",
      "Episode: 1753 steps: 9\n",
      "Episode: 1754 steps: 8\n",
      "Episode: 1755 steps: 10\n",
      "Episode: 1756 steps: 10\n",
      "Episode: 1757 steps: 10\n",
      "Episode: 1758 steps: 9\n",
      "Episode: 1759 steps: 9\n",
      "Episode: 1760 steps: 9\n",
      "Episode: 1761 steps: 8\n",
      "Episode: 1762 steps: 11\n",
      "Episode: 1763 steps: 9\n",
      "Episode: 1764 steps: 9\n",
      "Episode: 1765 steps: 10\n",
      "Episode: 1766 steps: 9\n",
      "Episode: 1767 steps: 10\n",
      "Episode: 1768 steps: 11\n",
      "Episode: 1769 steps: 9\n",
      "Episode: 1770 steps: 9\n",
      "Episode: 1771 steps: 9\n",
      "Episode: 1772 steps: 10\n",
      "Episode: 1773 steps: 8\n",
      "Episode: 1774 steps: 9\n",
      "Episode: 1775 steps: 9\n",
      "Episode: 1776 steps: 9\n",
      "Episode: 1777 steps: 9\n",
      "Episode: 1778 steps: 10\n",
      "Episode: 1779 steps: 9\n",
      "Episode: 1780 steps: 8\n",
      "Episode: 1781 steps: 8\n",
      "Episode: 1782 steps: 10\n",
      "Episode: 1783 steps: 10\n",
      "Episode: 1784 steps: 9\n",
      "Episode: 1785 steps: 9\n",
      "Episode: 1786 steps: 10\n",
      "Episode: 1787 steps: 10\n",
      "Episode: 1788 steps: 10\n",
      "Episode: 1789 steps: 8\n",
      "Episode: 1790 steps: 9\n",
      "Episode: 1791 steps: 8\n",
      "Episode: 1792 steps: 10\n",
      "Episode: 1793 steps: 10\n",
      "Episode: 1794 steps: 10\n",
      "Episode: 1795 steps: 11\n",
      "Episode: 1796 steps: 10\n",
      "Episode: 1797 steps: 9\n",
      "Episode: 1798 steps: 9\n",
      "Episode: 1799 steps: 9\n",
      "Episode: 1800 steps: 9\n",
      "Episode: 1801 steps: 10\n",
      "Episode: 1802 steps: 10\n",
      "Episode: 1803 steps: 9\n",
      "Episode: 1804 steps: 10\n",
      "Episode: 1805 steps: 9\n",
      "Episode: 1806 steps: 9\n",
      "Episode: 1807 steps: 9\n",
      "Episode: 1808 steps: 9\n",
      "Episode: 1809 steps: 10\n",
      "Episode: 1810 steps: 10\n",
      "Episode: 1811 steps: 8\n",
      "Episode: 1812 steps: 10\n",
      "Episode: 1813 steps: 10\n",
      "Episode: 1814 steps: 9\n",
      "Episode: 1815 steps: 8\n",
      "Episode: 1816 steps: 9\n",
      "Episode: 1817 steps: 9\n",
      "Episode: 1818 steps: 19\n",
      "Episode: 1819 steps: 9\n",
      "Episode: 1820 steps: 10\n",
      "Episode: 1821 steps: 9\n",
      "Episode: 1822 steps: 26\n",
      "Episode: 1823 steps: 10\n",
      "Episode: 1824 steps: 8\n",
      "Episode: 1825 steps: 9\n",
      "Episode: 1826 steps: 8\n",
      "Episode: 1827 steps: 9\n",
      "Episode: 1828 steps: 8\n",
      "Episode: 1829 steps: 9\n",
      "Episode: 1830 steps: 8\n",
      "Episode: 1831 steps: 10\n",
      "Episode: 1832 steps: 9\n",
      "Episode: 1833 steps: 10\n",
      "Episode: 1834 steps: 9\n",
      "Episode: 1835 steps: 9\n",
      "Episode: 1836 steps: 11\n",
      "Episode: 1837 steps: 9\n",
      "Episode: 1838 steps: 10\n",
      "Episode: 1839 steps: 10\n",
      "Episode: 1840 steps: 10\n",
      "Episode: 1841 steps: 10\n",
      "Episode: 1842 steps: 9\n",
      "Episode: 1843 steps: 9\n",
      "Episode: 1844 steps: 10\n",
      "Episode: 1845 steps: 11\n",
      "Episode: 1846 steps: 10\n",
      "Episode: 1847 steps: 9\n",
      "Episode: 1848 steps: 9\n",
      "Episode: 1849 steps: 10\n",
      "Episode: 1850 steps: 10\n",
      "Episode: 1851 steps: 10\n",
      "Episode: 1852 steps: 9\n",
      "Episode: 1853 steps: 9\n",
      "Episode: 1854 steps: 9\n",
      "Episode: 1855 steps: 9\n",
      "Episode: 1856 steps: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1857 steps: 10\n",
      "Episode: 1858 steps: 10\n",
      "Episode: 1859 steps: 9\n",
      "Episode: 1860 steps: 9\n",
      "Episode: 1861 steps: 10\n",
      "Episode: 1862 steps: 10\n",
      "Episode: 1863 steps: 10\n",
      "Episode: 1864 steps: 9\n",
      "Episode: 1865 steps: 9\n",
      "Episode: 1866 steps: 11\n",
      "Episode: 1867 steps: 9\n",
      "Episode: 1868 steps: 10\n",
      "Episode: 1869 steps: 10\n",
      "Episode: 1870 steps: 9\n",
      "Episode: 1871 steps: 9\n",
      "Episode: 1872 steps: 9\n",
      "Episode: 1873 steps: 10\n",
      "Episode: 1874 steps: 10\n",
      "Episode: 1875 steps: 10\n",
      "Episode: 1876 steps: 9\n",
      "Episode: 1877 steps: 10\n",
      "Episode: 1878 steps: 9\n",
      "Episode: 1879 steps: 9\n",
      "Episode: 1880 steps: 8\n",
      "Episode: 1881 steps: 9\n",
      "Episode: 1882 steps: 9\n",
      "Episode: 1883 steps: 9\n",
      "Episode: 1884 steps: 9\n",
      "Episode: 1885 steps: 10\n",
      "Episode: 1886 steps: 10\n",
      "Episode: 1887 steps: 9\n",
      "Episode: 1888 steps: 9\n",
      "Episode: 1889 steps: 10\n",
      "Episode: 1890 steps: 9\n",
      "Episode: 1891 steps: 9\n",
      "Episode: 1892 steps: 10\n",
      "Episode: 1893 steps: 9\n",
      "Episode: 1894 steps: 9\n",
      "Episode: 1895 steps: 10\n",
      "Episode: 1896 steps: 10\n",
      "Episode: 1897 steps: 8\n",
      "Episode: 1898 steps: 10\n",
      "Episode: 1899 steps: 9\n",
      "Episode: 1900 steps: 9\n",
      "Episode: 1901 steps: 9\n",
      "Episode: 1902 steps: 9\n",
      "Episode: 1903 steps: 8\n",
      "Episode: 1904 steps: 9\n",
      "Episode: 1905 steps: 9\n",
      "Episode: 1906 steps: 10\n",
      "Episode: 1907 steps: 9\n",
      "Episode: 1908 steps: 9\n",
      "Episode: 1909 steps: 11\n",
      "Episode: 1910 steps: 10\n",
      "Episode: 1911 steps: 8\n",
      "Episode: 1912 steps: 9\n",
      "Episode: 1913 steps: 9\n",
      "Episode: 1914 steps: 11\n",
      "Episode: 1915 steps: 9\n",
      "Episode: 1916 steps: 10\n",
      "Episode: 1917 steps: 9\n",
      "Episode: 1918 steps: 8\n",
      "Episode: 1919 steps: 9\n",
      "Episode: 1920 steps: 9\n",
      "Episode: 1921 steps: 9\n",
      "Episode: 1922 steps: 8\n",
      "Episode: 1923 steps: 10\n",
      "Episode: 1924 steps: 8\n",
      "Episode: 1925 steps: 9\n",
      "Episode: 1926 steps: 9\n",
      "Episode: 1927 steps: 8\n",
      "Episode: 1928 steps: 10\n",
      "Episode: 1929 steps: 9\n",
      "Episode: 1930 steps: 10\n",
      "Episode: 1931 steps: 8\n",
      "Episode: 1932 steps: 10\n",
      "Episode: 1933 steps: 10\n",
      "Episode: 1934 steps: 10\n",
      "Episode: 1935 steps: 10\n",
      "Episode: 1936 steps: 10\n",
      "Episode: 1937 steps: 8\n",
      "Episode: 1938 steps: 10\n",
      "Episode: 1939 steps: 9\n",
      "Episode: 1940 steps: 8\n",
      "Episode: 1941 steps: 9\n",
      "Episode: 1942 steps: 10\n",
      "Episode: 1943 steps: 10\n",
      "Episode: 1944 steps: 8\n",
      "Episode: 1945 steps: 12\n",
      "Episode: 1946 steps: 9\n",
      "Episode: 1947 steps: 8\n",
      "Episode: 1948 steps: 9\n",
      "Episode: 1949 steps: 10\n",
      "Episode: 1950 steps: 8\n",
      "Episode: 1951 steps: 9\n",
      "Episode: 1952 steps: 10\n",
      "Episode: 1953 steps: 11\n",
      "Episode: 1954 steps: 8\n",
      "Episode: 1955 steps: 9\n",
      "Episode: 1956 steps: 10\n",
      "Episode: 1957 steps: 9\n",
      "Episode: 1958 steps: 9\n",
      "Episode: 1959 steps: 10\n",
      "Episode: 1960 steps: 10\n",
      "Episode: 1961 steps: 9\n",
      "Episode: 1962 steps: 10\n",
      "Episode: 1963 steps: 9\n",
      "Episode: 1964 steps: 10\n",
      "Episode: 1965 steps: 8\n",
      "Episode: 1966 steps: 9\n",
      "Episode: 1967 steps: 10\n",
      "Episode: 1968 steps: 9\n",
      "Episode: 1969 steps: 10\n",
      "Episode: 1970 steps: 10\n",
      "Episode: 1971 steps: 10\n",
      "Episode: 1972 steps: 9\n",
      "Episode: 1973 steps: 10\n",
      "Episode: 1974 steps: 9\n",
      "Episode: 1975 steps: 10\n",
      "Episode: 1976 steps: 9\n",
      "Episode: 1977 steps: 9\n",
      "Episode: 1978 steps: 10\n",
      "Episode: 1979 steps: 9\n",
      "Episode: 1980 steps: 10\n",
      "Episode: 1981 steps: 11\n",
      "Episode: 1982 steps: 9\n",
      "Episode: 1983 steps: 10\n",
      "Episode: 1984 steps: 8\n",
      "Episode: 1985 steps: 8\n",
      "Episode: 1986 steps: 9\n",
      "Episode: 1987 steps: 10\n",
      "Episode: 1988 steps: 9\n",
      "Episode: 1989 steps: 10\n",
      "Episode: 1990 steps: 9\n",
      "Episode: 1991 steps: 8\n",
      "Episode: 1992 steps: 9\n",
      "Episode: 1993 steps: 9\n",
      "Episode: 1994 steps: 10\n",
      "Episode: 1995 steps: 8\n",
      "Episode: 1996 steps: 9\n",
      "Episode: 1997 steps: 8\n",
      "Episode: 1998 steps: 10\n",
      "Episode: 1999 steps: 9\n",
      "Total Score: 34.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_episodes):\n",
    "        e = 1. / ((i / 10) + 1)\n",
    "        rAll = 0\n",
    "        step_count = 0\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            step_count += 1\n",
    "            x = np.reshape(s, [1, input_size])\n",
    "            Qs = sess.run(Qpred, feed_dict={X: x})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                np.argmax(Qs)\n",
    "            s1, reward, done, _ = env.step(a)\n",
    "            if done:\n",
    "                Qs[0, a] = -100\n",
    "            else:\n",
    "                x1 = np.reshape(s1, [1, input_size])\n",
    "                Qs1 = sess.run(Qpred, feed_dict={X: x1})\n",
    "                Qs[0, a] = reward + dis * np.max(Qs1)\n",
    "            sess.run(train, feed_dict={X: x, Y: Qs})\n",
    "            s = s1\n",
    "\n",
    "        rList.append(step_count)\n",
    "        print(\"Episode: {} steps: {}\".format(i, step_count))\n",
    "        if len(rList) > 10 and np.mean(rList[-10:]) > 500:\n",
    "            break\n",
    "            \n",
    "    observation = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        x = np.reshape(observation, [1, input_size])\n",
    "        Qs = sess.run(Qpred, feed_dict={X: x})\n",
    "        a = np.argmax(Qs)\n",
    "\n",
    "        observation, reward, done, _ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(\"Total Score: {}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
